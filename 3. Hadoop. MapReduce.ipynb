{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop and MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azureuser\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В своей сути MapRedcue это очень простая парадигма. Допустим у нас есть датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:25:44.158742Z",
     "start_time": "2021-01-25T20:25:41.796781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  20.1M      0  0:00:04  0:00:04 --:--:-- 27.7M\n"
     ]
    }
   ],
   "source": [
    "! curl https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv > tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим в этом датасете что-нибудь найти. Например (сейчас будет баян), посчитать количество уникальных слов. Мы могли бы сделать что-то такое:\n",
    "\n",
    "#### Вариант 1 \n",
    "Используем исключительно питон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in ./anaconda3/lib/python3.8/site-packages (0.58.0)\n",
      "Requirement already satisfied: psutil in ./anaconda3/lib/python3.8/site-packages (from memory_profiler) (5.7.2)\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "! pip install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:27.659125Z",
     "start_time": "2021-01-25T20:30:25.819459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\n",
      "co\t250375\n",
      "https\t221366\n",
      "the\t69350\n",
      "to\t55972\n",
      "a\t43420\n",
      "in\t37099\n",
      "s\t36085\n",
      "of\t33579\n",
      "http\t28661\n",
      "Memory 19470488\n",
      "peak memory: 90.38 MiB, increment: 9.68 MiB\n",
      "CPU times: user 10.6 s, sys: 1.27 s, total: 11.9 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "\n",
    "counter = Counter()\n",
    "pattern = re.compile(r\"[a-z]+\")\n",
    "\n",
    "with open('tweets_1.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "\n",
    "for word, count in counter.most_common(10):\n",
    "    print(f\"{word}\\t{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:53.751130Z",
     "start_time": "2021-01-25T20:30:53.746887Z"
    }
   },
   "source": [
    "Такое сработает только если у нас не очень много данных и они все вмещаются в оперативную память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90M\ttweets_1.csv\r\n"
     ]
    }
   ],
   "source": [
    "! du -h tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант 2\n",
    "Используем парадигму Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере у нас всего 90 мегабайт данных, и на моем компьютере они обрабатываются за примерно 30 секунд с помощью питона. Теперь представим (это достаточно несложно), что у нас приходит новых данных приходит _десятки терабайт_ в сутки. Такое уже не поместится ни в один сервер, поэтому нам нужно придумать что-нибудь похитрее.\n",
    "\n",
    "MapReduce как раз является парадигмой, помогающей обрабатывать большие объемы данных, за счет простоты своего устройства.\n",
    "\n",
    "Приятная новость - для того, чтобы понять и научиться программировать программы в парадигме MapReduce вам потребуется... **5 секунд!**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/imgs/you-know-mapreduce.png\" width=\"400\">\n",
    "\n",
    "Все потому что вы уже прошли семинар по Bash и научились составлять большие программы в виде компоновки небольших  программ, соединенных пайпами. По своей сути программа на MapReduce - это хорошо отмасштабированная программа вида.\n",
    "\n",
    "```bash\n",
    "cat data.txt | map | sort | reduce\n",
    "```\n",
    "\n",
    "Сортировку за вас выполняет сам фреймворк (и ее вы можете дополнительно настроить точно такое как и команду sort). А также он самостоятельно разобьем данные на части и параллельно запустит операции map и reduce. \n",
    "\n",
    "Таким образом на самом деле Hadoop - это всего лишь гигантская машина сортировки, которая дополнительно дает вам некоторые гарантии:\n",
    "\n",
    "* Для всех данных параллельно будет применена операция map\n",
    "* Данные будут отсортированы по указанному вами ключу\n",
    "* Каждый ключ будет целиком передан на один и только один reduce\n",
    "\n",
    "Программисту остается реализовать программу, которая состоит из двух компонент: `map` и `reduce`. \n",
    "\n",
    "Операция `map` -- это просто функция из одного элемента в другой элемент, у которого есть первичный ключ. \n",
    "\n",
    "Операция `reduce` -- это коммутативная и ассоциативная агрегация всех элементов по ключу. Чтобы эти операции совершить, надо разбить весь вход на куски данных и отправить их на машины, чтобы они выполнялись в параллель, а весь выход операции map идёт в операцию shuffle, которая по одним и тем же ключам определяет записи на одинаковые хосты. \n",
    "\n",
    "В итоге получается, что мы можем спокойно увеличивать количество worker'ов для map операций и с увеличением количества данных мы лишь будем линейно утилизировать количество машин, то же самое с операцией reduce -- мы можем добавлять машины с ростом увеличения количества ключей линейно, не боясь того, что мы не можем позволить на одной какой-то машине больше памяти или диска.\n",
    "\n",
    "Давайте напишем маппер и редьюсер на питоне для этой задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:45:38.075819Z",
     "start_time": "2021-01-25T20:45:38.069953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    word, number = next(sys.stdin).split('\\t')\n",
    "    number = int(number)\n",
    "    for line in sys.stdin:\n",
    "        current_word, current_number = line.split('\\t')\n",
    "        current_number = int(current_number)\n",
    "        if current_word != word:\n",
    "            print(\"{}\\t{}\".format(word, number))\n",
    "            word = current_word\n",
    "            number = current_number\n",
    "        else:\n",
    "            number += current_number\n",
    "    print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно еще удалить голову у таблицы, иначе подсчеты могут быть некоректными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1'd tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "! cat tweets_1.csv | python wordcount.py map | sort -k1,1 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:47:23.728893Z",
     "start_time": "2021-01-25T20:46:51.895208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 243891/243891 [00:26<00:00, 9377.95it/s]\n",
      "CPU times: user 565 ms, sys: 67.9 ms, total: 633 ms\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! cat tweets_1.csv | \\\n",
    "    tqdm --total $(cat tweets_1.csv | wc -l)| \\\n",
    "    python wordcount.py map | \\\n",
    "    sort -k1,1 | \\\n",
    "    python wordcount.py reduce > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t43420\r\n",
      "aa\t151\r\n",
      "aaa\t13\r\n",
      "aaaaaa\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaaall\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaaand\t2\r\n",
      "aaaaaaargh\t1\r\n",
      "aaaaaand\t2\r\n"
     ]
    }
   ],
   "source": [
    "! head result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:50:28.963822Z",
     "start_time": "2021-01-25T20:50:28.265568Z"
    }
   },
   "source": [
    "Отлично! Слова есть, осталось только найти top-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10.py\n",
    "import sys\n",
    "\n",
    "\n",
    "def _rewind_stream(stream):\n",
    "    for _ in stream:\n",
    "        pass\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in sys.stdin:\n",
    "        key, value = row.split('\\t')\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for _ in range(10):\n",
    "        key, _ = next(sys.stdin).split('\\t')\n",
    "        word, count = key.split(\"+\")\n",
    "        print(\"{}\\t{}\".format(word, count))\n",
    "    _rewind_stream(sys.stdin)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 346613/346613 [00:01<00:00, 275994.94it/s]\n"
     ]
    }
   ],
   "source": [
    "! cat result.txt | \\\n",
    "    tqdm --total $(cat result.txt | wc -l) | \\\n",
    "    python top10.py map | \\\n",
    "    sort -t'+' -k2,2nr -k1,1 | \\\n",
    "    python top10.py reduce > top-10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\r\n",
      "co\t250375\r\n",
      "https\t221366\r\n",
      "the\t69350\r\n",
      "to\t55972\r\n",
      "a\t43420\r\n",
      "in\t37099\r\n",
      "s\t36085\r\n",
      "of\t33579\r\n",
      "http\t28661\r\n"
     ]
    }
   ],
   "source": [
    "! cat top-10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На MapReduce мы задачу переписали, однако быстрее работать она пока не стала. Все дело в том, что мы это еще не на кластере запускали! Время запускать все на настоящем кластере!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала давайте сохраним где-нибудь наш пароль от ажура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:14:36.933488Z",
     "start_time": "2021-01-25T19:14:31.404805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "9"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import getpass\n",
    "password = getpass.getpass()\n",
    "\n",
    "with pathlib.Path('~/.azure_password').expanduser().open('w') as file:\n",
    "    file.write(password + '\\n')\n",
    "\n",
    "%cat ~/.azure_password | head -c 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:15:54.035411Z",
     "start_time": "2021-01-25T19:15:54.030542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/naorlov/Dropbox/hse/teaching/2020-lsml/lsml-2021-internal/cloud-configuration\n"
     ]
    }
   ],
   "source": [
    "%cd cloud-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hadoop.tf\n",
    "\n",
    "resource \"azurerm_storage_container\" \"lsml_hadoop_sc\" {\n",
    "  name                  = \"lsmlhdinsight\"\n",
    "  storage_account_name  = azurerm_storage_account.lsml_sa.name\n",
    "  container_access_type = \"private\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\" {\n",
    "  name                = \"lsml-hdicluster\"\n",
    "  resource_group_name = azurerm_resource_group.lsml_rg.name\n",
    "  location            = azurerm_resource_group.lsml_rg.location\n",
    "  cluster_version     = \"4.0\"\n",
    "  tier                = \"Standard\"\n",
    "\n",
    "  component_version {\n",
    "    hadoop = \"3.1\"\n",
    "  }\n",
    "\n",
    "  gateway {\n",
    "    enabled  = true\n",
    "    username = \"azureuser\"\n",
    "    password = \"Password123!\"\n",
    "  }\n",
    "\n",
    "  storage_account {\n",
    "    storage_container_id = azurerm_storage_container.lsml_hadoop_sc.id\n",
    "    storage_account_key  = azurerm_storage_account.lsml_sa.primary_access_key\n",
    "    is_default           = true\n",
    "  }\n",
    "\n",
    "  roles {\n",
    "    head_node {\n",
    "      vm_size  = \"A5\"  # 2 cpu 4 ram\n",
    "      username = \"azureuser\"\n",
    "      password = \"Password123!\"\n",
    "    }\n",
    "\n",
    "    worker_node {\n",
    "      vm_size               = \"Standard_D12_V2\" # 4 cpu 28 ram\n",
    "      username              = \"azureuser\"\n",
    "      password              = \"Password123!\"\n",
    "      target_instance_count = 2\n",
    "    }\n",
    "\n",
    "    zookeeper_node {\n",
    "      vm_size  = \"Standard_A2_V2\"  # 2 cpu 4 ram\n",
    "      username = \"azureuser\"\n",
    "      password = \"Password123!s\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "output \"ssh_endpoint\" {\n",
    "    value = azurerm_hdinsight_hadoop_cluster.lsml_hc.ssh_endpoint\n",
    "}\n",
    "\n",
    "output \"https_endpoint\" {\n",
    "    value = azurerm_hdinsight_hadoop_cluster.lsml_hc.https_endpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting common.tf\n"
     ]
    }
   ],
   "source": [
    "%%writefile common.tf\n",
    "\n",
    "provider \"azurerm\" {\n",
    "  version = \"=2.40.0\"\n",
    "  features {}\n",
    "}\n",
    "\n",
    "resource \"azurerm_resource_group\" \"lsml_rg\" {\n",
    "  name = \"lsml-resource-group\"\n",
    "  location = \"westus\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_virtual_network\" \"lsml_vn\" {\n",
    "  name = \"lsml-vitrual-network\"\n",
    "  resource_group_name = azurerm_resource_group.lsml_rg.name\n",
    "  location = azurerm_resource_group.lsml_rg.location\n",
    "  address_space = [\"10.0.0.0/16\"]   # Пул адресов внутри сети\n",
    "}\n",
    "\n",
    "resource \"azurerm_storage_account\" \"lsml_sa\" {\n",
    "  name                     = \"lsmlhdinsightstore\"\n",
    "  resource_group_name      = azurerm_resource_group.lsml_rg.name\n",
    "  location                 = azurerm_resource_group.lsml_rg.location\n",
    "  account_tier             = \"Standard\"\n",
    "  account_replication_type = \"LRS\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common.tf  hadoop.tf\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:16:03.616451Z",
     "start_time": "2021-01-25T19:16:03.492991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/naorlov/Dropbox/hse/teaching/2020-lsml/lsml-2021-internal/cloud-configuration\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:18:23.042407Z",
     "start_time": "2021-01-25T19:18:22.917762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: terraform\r\n"
     ]
    }
   ],
   "source": [
    "! terraform init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-06 15:04:36--  https://aka.ms/InstallAzureCLIDeb\n",
      "Resolving aka.ms (aka.ms)... 23.53.53.76\n",
      "Connecting to aka.ms (aka.ms)|23.53.53.76|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://azurecliprod.blob.core.windows.net/$root/deb_install.sh [following]\n",
      "--2021-01-06 15:04:37--  https://azurecliprod.blob.core.windows.net/$root/deb_install.sh\n",
      "Resolving azurecliprod.blob.core.windows.net (azurecliprod.blob.core.windows.net)... 13.88.145.64\n",
      "Connecting to azurecliprod.blob.core.windows.net (azurecliprod.blob.core.windows.net)|13.88.145.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4309 (4.2K) [text/x-sh]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]   4.21K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-01-06 15:04:38 (309 MB/s) - written to stdout [4309/4309]\n",
      "\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [109 kB]      \n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [101 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1,275 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [109 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [516 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.0 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [897 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [145 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [916 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [4,250 B]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [646 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [741 B]\n",
      "Fetched 16.7 MB in 2s (6,686 kB/s)              \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  dirmngr distro-info-data gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client\n",
      "  gpg-wks-server gpgconf gpgsm libassuan0 libcurl4 libksba8 libnpth0\n",
      "  pinentry-curses\n",
      "Suggested packages:\n",
      "  dbus-user-session libpam-systemd pinentry-gnome3 tor parcimonie xloadimage\n",
      "  scdaemon pinentry-doc\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https curl dirmngr distro-info-data gnupg gnupg-l10n\n",
      "  gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm\n",
      "  libassuan0 libcurl4 libksba8 libnpth0 lsb-release pinentry-curses\n",
      "0 upgraded, 19 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 2,948 kB of archives.\n",
      "After this operation, 8,850 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 distro-info-data all 0.43ubuntu1.4 [4,624 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 lsb-release all 11.1.0ubuntu2 [10.6 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.2ubuntu0.2 [1,708 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcurl4 amd64 7.68.0-1ubuntu2.4 [234 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 curl amd64 7.68.0-1ubuntu2.4 [161 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libassuan0 amd64 2.5.3-7ubuntu2 [35.7 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 gpgconf amd64 2.2.19-3ubuntu2 [124 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libksba8 amd64 1.3.5-2 [92.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libnpth0 amd64 1.6-1 [7,736 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 dirmngr amd64 2.2.19-3ubuntu2 [329 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 gnupg-l10n all 2.2.19-3ubuntu2 [51.7 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 gnupg-utils amd64 2.2.19-3ubuntu2 [481 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 gpg amd64 2.2.19-3ubuntu2 [482 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 pinentry-curses amd64 1.1.0-3build1 [36.3 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 gpg-agent amd64 2.2.19-3ubuntu2 [232 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 gpg-wks-client amd64 2.2.19-3ubuntu2 [97.7 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 gpg-wks-server amd64 2.2.19-3ubuntu2 [90.3 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 gpgsm amd64 2.2.19-3ubuntu2 [217 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 gnupg all 2.2.19-3ubuntu2 [259 kB]\n",
      "Fetched 2,948 kB in 1s (4,384 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package distro-info-data.\n",
      "(Reading database ... 62119 files and directories currently installed.)\n",
      "Preparing to unpack .../00-distro-info-data_0.43ubuntu1.4_all.deb ...\n",
      "Unpacking distro-info-data (0.43ubuntu1.4) ...\n",
      "Selecting previously unselected package lsb-release.\n",
      "Preparing to unpack .../01-lsb-release_11.1.0ubuntu2_all.deb ...\n",
      "Unpacking lsb-release (11.1.0ubuntu2) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "Preparing to unpack .../02-apt-transport-https_2.0.2ubuntu0.2_all.deb ...\n",
      "Unpacking apt-transport-https (2.0.2ubuntu0.2) ...\n",
      "Selecting previously unselected package libcurl4:amd64.\n",
      "Preparing to unpack .../03-libcurl4_7.68.0-1ubuntu2.4_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.68.0-1ubuntu2.4) ...\n",
      "Selecting previously unselected package curl.\n",
      "Preparing to unpack .../04-curl_7.68.0-1ubuntu2.4_amd64.deb ...\n",
      "Unpacking curl (7.68.0-1ubuntu2.4) ...\n",
      "Selecting previously unselected package libassuan0:amd64.\n",
      "Preparing to unpack .../05-libassuan0_2.5.3-7ubuntu2_amd64.deb ...\n",
      "Unpacking libassuan0:amd64 (2.5.3-7ubuntu2) ...\n",
      "Selecting previously unselected package gpgconf.\n",
      "Preparing to unpack .../06-gpgconf_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../07-libksba8_1.3.5-2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.3.5-2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../08-libnpth0_1.6-1_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-1) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../09-dirmngr_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../10-gnupg-l10n_2.2.19-3ubuntu2_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../11-gnupg-utils_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package gpg.\n",
      "Preparing to unpack .../12-gpg_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking gpg (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../13-pinentry-curses_1.1.0-3build1_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.0-3build1) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../14-gpg-agent_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../15-gpg-wks-client_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../16-gpg-wks-server_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../17-gpgsm_2.2.19-3ubuntu2_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.19-3ubuntu2) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../18-gnupg_2.2.19-3ubuntu2_all.deb ...\n",
      "Unpacking gnupg (2.2.19-3ubuntu2) ...\n",
      "Setting up libksba8:amd64 (1.3.5-2) ...\n",
      "Setting up apt-transport-https (2.0.2ubuntu0.2) ...\n",
      "Setting up distro-info-data (0.43ubuntu1.4) ...\n",
      "Setting up libnpth0:amd64 (1.6-1) ...\n",
      "Setting up libassuan0:amd64 (2.5.3-7ubuntu2) ...\n",
      "Setting up gnupg-l10n (2.2.19-3ubuntu2) ...\n",
      "Setting up gpgconf (2.2.19-3ubuntu2) ...\n",
      "Setting up libcurl4:amd64 (7.68.0-1ubuntu2.4) ...\n",
      "Setting up curl (7.68.0-1ubuntu2.4) ...\n",
      "Setting up lsb-release (11.1.0ubuntu2) ...\n",
      "Setting up gpg (2.2.19-3ubuntu2) ...\n",
      "Setting up gnupg-utils (2.2.19-3ubuntu2) ...\n",
      "Setting up pinentry-curses (1.1.0-3build1) ...\n",
      "Setting up gpg-agent (2.2.19-3ubuntu2) ...\n",
      "Setting up gpgsm (2.2.19-3ubuntu2) ...\n",
      "Setting up dirmngr (2.2.19-3ubuntu2) ...\n",
      "Setting up gpg-wks-server (2.2.19-3ubuntu2) ...\n",
      "Setting up gpg-wks-client (2.2.19-3ubuntu2) ...\n",
      "Setting up gnupg (2.2.19-3ubuntu2) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.1) ...\n",
      "Processing triggers for install-info (6.7.0.dfsg.2-5) ...\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease               \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease                 \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease               \n",
      "Get:5 https://packages.microsoft.com/repos/azure-cli focal InRelease [10.4 kB]\n",
      "Get:6 https://packages.microsoft.com/repos/azure-cli focal/main amd64 Packages [3,861 B]\n",
      "Fetched 14.3 kB in 1s (21.3 kB/s)    \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  azure-cli\n",
      "0 upgraded, 1 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 57.2 MB of archives.\n",
      "After this operation, 756 MB of additional disk space will be used.\n",
      "Get:1 https://packages.microsoft.com/repos/azure-cli focal/main amd64 azure-cli all 2.17.1-1~focal [57.2 MB]\n",
      "Fetched 57.2 MB in 6s (9,107 kB/s)                                             \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package azure-cli.\n",
      "(Reading database ... 62367 files and directories currently installed.)\n",
      "Preparing to unpack .../azure-cli_2.17.1-1~focal_all.deb ...\n",
      "Unpacking azure-cli (2.17.1-1~focal) ...\n",
      "Setting up azure-cli (2.17.1-1~focal) ...\n"
     ]
    }
   ],
   "source": [
    "! wget https://aka.ms/InstallAzureCLIDeb -O - | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code DF3WP52LX to authenticate.\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"21f26c24-0793-4b07-a73d-563cd2ec235f\",\n",
      "    \"id\": \"7d1225ca-27cc-40b7-8036-c62a48072ba8\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [],\n",
      "    \"name\": \"Спонсорское предложение Microsoft Azure 2\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"21f26c24-0793-4b07-a73d-563cd2ec235f\",\n",
      "    \"user\": {\n",
      "      \"name\": \"adkosmachev@edu.hse.ru\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_resource_group.lsml_rg: Refreshing state... [id=/subscriptions/7d1225ca-27cc-40b7-8036-c62a48072ba8/resourceGroups/lsml-resource-group]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_virtual_network.lsml_vn: Refreshing state... [id=/subscriptions/7d1225ca-27cc-40b7-8036-c62a48072ba8/resourceGroups/lsml-resource-group/providers/Microsoft.Network/virtualNetworks/lsml-vitrual-network]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Refreshing state... [id=/subscriptions/7d1225ca-27cc-40b7-8036-c62a48072ba8/resourceGroups/lsml-resource-group/providers/Microsoft.Storage/storageAccounts/lsmlhdinsightstore]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_container.lsml_hadoop_sc: Refreshing state... [id=https://lsmlhdinsightstore.blob.core.windows.net/lsmlhdinsight]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Refreshing state... [id=/subscriptions/7d1225ca-27cc-40b7-8036-c62a48072ba8/resourceGroups/lsml-resource-group/providers/Microsoft.HDInsight/clusters/lsml-hdicluster]\u001b[0m\n",
      "\n",
      "An execution plan has been generated and is shown below.\n",
      "Resource actions are indicated with the following symbols:\n",
      "\n",
      "Terraform will perform the following actions:\n",
      "\n",
      "\u001b[0m\u001b[1mPlan:\u001b[0m 0 to add, 0 to change, 0 to destroy.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mChanges to Outputs:\u001b[0m\n",
      "  \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mhttps_endpoint\u001b[0m\u001b[0m = \"lsml-hdicluster.azurehdinsight.net\"\u001b[0m\n",
      "\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1mVersion constraints inside provider configuration blocks are deprecated\u001b[0m\n",
      "\n",
      "\u001b[0m  on common.tf line 3, in provider \"azurerm\":\n",
      "   3:   version = \u001b[4m\"=2.40.0\"\u001b[0m\n",
      "\u001b[0m\n",
      "Terraform 0.13 and earlier allowed provider version constraints inside the\n",
      "provider configuration block, but that is now deprecated and will be removed\n",
      "in a future version of Terraform. To silence this warning, move the provider\n",
      "version constraint into the required_providers block.\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1m\"gateway.0.enabled\": [DEPRECATED] HDInsight doesn't support disabling gateway anymore\u001b[0m\n",
      "\n",
      "\u001b[0m  on hadoop.tf line 8, in resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\":\n",
      "   8: resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\" \u001b[4m{\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mDo you want to perform these actions?\u001b[0m\n",
      "  Terraform will perform the actions described above.\n",
      "  Only 'yes' will be accepted to approve.\n",
      "\n",
      "  \u001b[1mEnter a value:\u001b[0m \u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Apply complete! Resources: 0 added, 0 changed, 0 destroyed.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Outputs:\n",
      "\n",
      "https_endpoint = \"lsml-hdicluster.azurehdinsight.net\"\n",
      "ssh_endpoint = \"lsml-hdicluster-ssh.azurehdinsight.net\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! echo \"yes\" | terraform apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https_endpoint = \"lsml-hdicluster.azurehdinsight.net\"\r\n",
      "ssh_endpoint = \"lsml-hdicluster-ssh.azurehdinsight.net\"\r\n"
     ]
    }
   ],
   "source": [
    "! terraform output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открываем lsml-hdicluster.azurehdinsight.net в браузере и вводим логин\\пароль от гейтвея.\n",
    "\n",
    "Мы попадем в интерфейс Ambari, где можно посмотреть состояние кластера и обновить какие-то параметры.\n",
    "\n",
    "Мы также можем подключиться к головной машине, используя ssh_endpoint\n",
    "\n",
    "```bash\n",
    "ssh azureuser@lsml-hdicluster-ssh.azurehdinsight.net\n",
    "```\n",
    "\n",
    "На самой машине можете запустить команду\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /\n",
    "\n",
    "\n",
    "Found 18 items\n",
    "-rwxrwxrwx   1                            0 2021-01-06 15:31 /HDInsight_TestAccessiblityBlobName\n",
    "drwxr-xr-x   - root   supergroup          0 2021-01-06 16:01 /HdiSamples\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:32 /ams\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:32 /amshbase\n",
    "drwxrwxrwx   - yarn   hadoop              0 2021-01-06 15:32 /app-logs\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:32 /apps\n",
    "drwxr-xr-x   - yarn   hadoop              0 2021-01-06 15:32 /atshistory\n",
    "drwxr-xr-x   - root   supergroup          0 2021-01-06 15:58 /custom-scriptaction-logs\n",
    "drwxr-xr-x   - root   supergroup          0 2021-01-06 15:58 /example\n",
    "drwxr-xr-x   - hbase  supergroup          0 2021-01-06 15:32 /hbase\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:32 /hdp\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:32 /hive\n",
    "drwxr-xr-x   - mapred supergroup          0 2021-01-06 15:32 /mapred\n",
    "drwxrwxrwx   - mapred hadoop              0 2021-01-06 15:32 /mr-history\n",
    "drwxrwxrwx   - hdfs   supergroup          0 2021-01-06 15:32 /tmp\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:32 /user\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:32 /warehouse\n",
    "drwxr-xr-x   - hdfs   supergroup          0 2021-01-06 15:42 /yarn\n",
    "\n",
    "```\n",
    "\n",
    "Вам отобразится состояние HDFS хранилища. Важно отметить, что это специальное HDFS хранилище, подключенное к WASB.\n",
    "Таким образом все результаты работы на кластере автоматически сохраняются в облачное хранилище и вы можете получить к ним доступ не только из Hadoop (проверьте и откройте соответствующий контейнер в браузере). \n",
    "\n",
    "Чтобы узнать приватный IP адрес головной машины, к которой нужно подключаться (через прокси) можно запустить команду \n",
    "\n",
    "```bash\n",
    "ifconfig\n",
    "```\n",
    "\n",
    "на самой машине. \n",
    "\n",
    "Дальнейшие команды семинара нужно будет запускать именно в облаке. Это можно делать просто через терминал, а можно и поднять там Jupyter и работать через него (как это делалось в первом семинаре)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные в HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:52:22.492958Z",
     "start_time": "2021-01-25T20:52:22.382757Z"
    }
   },
   "source": [
    "**ВАЖНО**: При следующем создании кластера нужно указать ту же самую запись хранения и тот же самый контейнер, чтобы все данные, с которомы вы работали в HDFS сохранились и вы могли продожить с ними работу.\n",
    "\n",
    "Посмотреть на ваши данные в HDFS можно через BLOB-storage view в самом Azure.\n",
    "\n",
    "Подгрузим данные с твитами в хадуп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv --> IRAhandle_tweets_1.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  99.3M      0 --:--:-- --:--:-- --:--:-- 99.2M\n",
      "\n",
      "[2/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv --> IRAhandle_tweets_2.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  16.5M      0  0:00:05  0:00:05 --:--:-- 20.3M\n",
      "\n",
      "[3/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv --> IRAhandle_tweets_3.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  24.5M      0  0:00:03  0:00:03 --:--:-- 24.5M\n",
      "\n",
      "[4/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv --> IRAhandle_tweets_4.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  27.1M      0  0:00:03  0:00:03 --:--:-- 27.1M\n",
      "\n",
      "[5/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv --> IRAhandle_tweets_5.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  25.9M      0  0:00:03  0:00:03 --:--:-- 25.9M\n",
      "\n",
      "[6/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv --> IRAhandle_tweets_6.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  25.8M      0  0:00:03  0:00:03 --:--:-- 25.8M\n",
      "\n",
      "[7/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv --> IRAhandle_tweets_7.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  28.4M      0  0:00:03  0:00:03 --:--:-- 28.4M\n",
      "\n",
      "[8/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv --> IRAhandle_tweets_8.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  28.9M      0  0:00:03  0:00:03 --:--:-- 28.8M\n",
      "\n",
      "[9/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv --> IRAhandle_tweets_9.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  19.6M      0  0:00:04  0:00:04 --:--:-- 19.6M\n",
      "\n",
      "[10/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv --> IRAhandle_tweets_10.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  24.9M      0  0:00:03  0:00:03 --:--:-- 24.9M\n",
      "\n",
      "[11/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv --> IRAhandle_tweets_11.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  25.4M      0  0:00:03  0:00:03 --:--:-- 25.4M\n",
      "\n",
      "[12/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv --> IRAhandle_tweets_12.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  27.8M      0  0:00:03  0:00:03 --:--:-- 27.8M\n",
      "\n",
      "[13/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv --> IRAhandle_tweets_13.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8045k  100 8045k    0     0  7633k      0  0:00:01  0:00:01 --:--:-- 7633k\n"
     ]
    }
   ],
   "source": [
    "! curl -O https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_{`seq -s , 1 13`}.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подформатируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1\n",
      "Finish 2\n",
      "Finish 3\n",
      "Finish 4\n",
      "Finish 5\n",
      "Finish 6\n",
      "Finish 7\n",
      "Finish 8\n",
      "Finish 9\n",
      "Finish 10\n",
      "Finish 11\n",
      "Finish 12\n",
      "Finish 13\n"
     ]
    }
   ],
   "source": [
    "! for i in {1..13}; do sed IRAhandle_tweets_$i.csv -i -e '1'd && echo \"Finish $i\" ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим отдельную папку для этих данных в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 items\n",
      "-rwxrwxrwx   1                               0 2021-01-27 06:46 /HDInsight_TestAccessiblityBlobName\n",
      "drwxr-xr-x   - root      supergroup          0 2021-01-26 18:51 /HdiSamples\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-27 06:47 /ams\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-27 06:47 /amshbase\n",
      "drwxrwxrwx   - yarn      hadoop              0 2021-01-27 06:47 /app-logs\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-27 06:47 /apps\n",
      "drwxr-xr-x   - yarn      hadoop              0 2021-01-27 06:47 /atshistory\n",
      "drwxr-xr-x   - root      supergroup          0 2021-01-26 18:46 /custom-scriptaction-logs\n",
      "drwxr-xr-x   - root      supergroup          0 2021-01-26 18:48 /example\n",
      "drwxr-xr-x   - hbase     supergroup          0 2021-01-27 06:47 /hbase\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-27 06:47 /hdp\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-27 06:47 /hive\n",
      "drwxr-xr-x   - mapred    supergroup          0 2021-01-27 06:47 /mapred\n",
      "drwx------   - azureuser supergroup          0 2021-01-26 20:20 /mapreducestaging\n",
      "drwxrwxrwx   - mapred    hadoop              0 2021-01-27 06:47 /mr-history\n",
      "drwxrwxrwx   - hdfs      supergroup          0 2021-01-27 06:47 /tmp\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-27 06:47 /user\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-27 06:47 /warehouse\n",
      "drwxr-xr-x   - hdfs      supergroup          0 2021-01-26 18:31 /yarn\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tweets/data': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /tweets/data\n",
    "! hdfs dfs -mkdir -p /tweets/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: все команды для hdfs смотреть здесь - https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Заливаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/01/27 12:10:35 WARN impl.MetricsSinkAdapter: azurefs2 has a full queue and can't consume the given metrics.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put IRAhandle_tweets_* /tweets/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 items\n",
      "-rw-r--r--   1 azureuser supergroup   94371561 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_1.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371615 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_10.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371552 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_11.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371703 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_12.csv\n",
      "-rw-r--r--   1 azureuser supergroup    8238864 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_13.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371748 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_2.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371796 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_3.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371606 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_4.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371616 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_5.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371646 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_6.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371711 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_7.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371727 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_8.csv\n",
      "-rw-r--r--   1 azureuser supergroup   94371542 2021-01-27 12:10 /tweets/data/IRAhandle_tweets_9.csv\n",
      "21/01/27 12:10:40 WARN impl.MetricsSinkAdapter: azurefs2 has a full queue and can't consume the given metrics.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tweets/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, что скрипты на головной машине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "import csv\r\n",
      "import re\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    pattern = re.compile(r\"[a-z]+\")\r\n",
      "    for row in csv.reader(iter(sys.stdin.readline, '')):\r\n",
      "        content = row[2]\r\n",
      "        for match in pattern.finditer(content.lower()):\r\n",
      "            word = match.group(0)\r\n",
      "            print(\"{}\\t{}\".format(word, 1))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    word, number = next(sys.stdin).split('\\t')\r\n",
      "    number = int(number)\r\n",
      "    for line in sys.stdin:\r\n",
      "        current_word, current_number = line.split('\\t')\r\n",
      "        current_number = int(current_number)\r\n",
      "        if current_word != word:\r\n",
      "            print(\"{}\\t{}\".format(word, number))\r\n",
      "            word = current_word\r\n",
      "            number = current_number\r\n",
      "        else:\r\n",
      "            number += current_number\r\n",
      "    print(\"{}\\t{}\".format(word, number))\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat wordcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "def _rewind_stream(stream):\r\n",
      "    for _ in stream:\r\n",
      "        pass\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    for row in sys.stdin:\r\n",
      "        key, value = row.split('\\t')\r\n",
      "        print(\"{}+{}\\t\".format(key, value.strip()))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    for _ in range(10):\r\n",
      "        key, _ = next(sys.stdin).split('\\t')\r\n",
      "        word, count = key.split(\"+\")\r\n",
      "        print(\"{}\\t{}\".format(word, count))\r\n",
      "    _rewind_stream(sys.stdin)\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat top10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем команду на запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tweets/result': No such file or directory\n",
      "WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\n",
      "packageJobJar: [] [/usr/hdp/4.1.2.5/hadoop/hadoop-streaming-3.1.3.4.1.2.5.jar] /tmp/streamjob2126156697770524973.jar tmpDir=null\n",
      "21/01/27 12:19:57 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:19:57 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:19:58 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:19:58 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:20:00 INFO client.RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\n",
      "21/01/27 12:20:00 INFO client.RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]\n",
      "21/01/27 12:20:02 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "21/01/27 12:20:02 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "21/01/27 12:20:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611730603892_0012\n",
      "21/01/27 12:20:03 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/01/27 12:20:04 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/4.1.2.5/0/resource-types.xml\n",
      "21/01/27 12:20:05 INFO impl.YarnClientImpl: Submitted application application_1611730603892_0012\n",
      "21/01/27 12:20:05 INFO mapreduce.Job: The url to track the job: http://hn1-lsml-h.bfsi4i1gtejurkfnybdpomj4ic.dx.internal.cloudapp.net:8088/proxy/application_1611730603892_0012/\n",
      "21/01/27 12:20:05 INFO mapreduce.Job: Running job: job_1611730603892_0012\n",
      "21/01/27 12:20:13 INFO mapreduce.Job: Job job_1611730603892_0012 running in uber mode : false\n",
      "21/01/27 12:20:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/27 12:20:24 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "21/01/27 12:20:25 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "21/01/27 12:20:26 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "21/01/27 12:20:27 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "21/01/27 12:20:28 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "21/01/27 12:20:29 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "21/01/27 12:20:30 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "21/01/27 12:20:31 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "21/01/27 12:20:32 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "21/01/27 12:20:33 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "21/01/27 12:20:34 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "21/01/27 12:20:35 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "21/01/27 12:20:36 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "21/01/27 12:20:37 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "21/01/27 12:20:38 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "21/01/27 12:20:39 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "21/01/27 12:20:40 INFO mapreduce.Job:  map 75% reduce 5%\n",
      "21/01/27 12:20:41 INFO mapreduce.Job:  map 82% reduce 10%\n",
      "21/01/27 12:20:43 INFO mapreduce.Job:  map 86% reduce 19%\n",
      "21/01/27 12:20:44 INFO mapreduce.Job:  map 87% reduce 21%\n",
      "21/01/27 12:20:48 INFO mapreduce.Job:  map 90% reduce 21%\n",
      "21/01/27 12:20:49 INFO mapreduce.Job:  map 92% reduce 21%\n",
      "21/01/27 12:20:50 INFO mapreduce.Job:  map 92% reduce 23%\n",
      "21/01/27 12:20:51 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "21/01/27 12:20:53 INFO mapreduce.Job:  map 100% reduce 37%\n",
      "21/01/27 12:20:54 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "21/01/27 12:20:56 INFO mapreduce.Job:  map 100% reduce 56%\n",
      "21/01/27 12:20:57 INFO mapreduce.Job:  map 100% reduce 63%\n",
      "21/01/27 12:20:59 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "21/01/27 12:21:00 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "21/01/27 12:21:02 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "21/01/27 12:21:03 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "21/01/27 12:21:05 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "21/01/27 12:21:06 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "21/01/27 12:21:07 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "21/01/27 12:21:09 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "21/01/27 12:21:10 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "21/01/27 12:21:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/27 12:21:13 INFO mapreduce.Job: Job job_1611730603892_0012 completed successfully\n",
      "21/01/27 12:21:13 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=397661430\n",
      "\t\tFILE: Number of bytes written=799312166\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tWASBS: Number of bytes read=1140713466\n",
      "\t\tWASBS: Number of bytes written=30359819\n",
      "\t\tWASBS: Number of read operations=0\n",
      "\t\tWASBS: Number of large read operations=0\n",
      "\t\tWASBS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tRack-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=356432\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=113431\n",
      "\t\tTotal time spent by all map tasks (ms)=356432\n",
      "\t\tTotal time spent by all reduce tasks (ms)=113431\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=356432\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=113431\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1094959104\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=348460032\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=397661646\n",
      "\t\tInput split bytes=2110\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=397661646\n",
      "\t\tReduce input records=41946754\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=83893508\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=3002\n",
      "\t\tCPU time spent (ms)=294620\n",
      "\t\tPhysical memory (bytes) snapshot=23203708928\n",
      "\t\tVirtual memory (bytes) snapshot=71771639808\n",
      "\t\tTotal committed heap usage (bytes)=42354081792\n",
      "\t\tPeak Map Physical memory (bytes)=1686519808\n",
      "\t\tPeak Map Virtual memory (bytes)=4513214464\n",
      "\t\tPeak Reduce Physical memory (bytes)=598003712\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4542115840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1140698687\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "21/01/27 12:21:13 INFO streaming.StreamJob: Output directory: /tweets/result/\n",
      "CPU times: user 1.35 s, sys: 372 ms, total: 1.72 s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /tweets/result || true\n",
    "! yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount.py \\\n",
    "-mapper \"python3 wordcount.py map\" \\\n",
    "-reducer \"python3 wordcount.py reduce\" \\\n",
    "-input /tweets/data/ \\\n",
    "-output /tweets/result/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим результат\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   1 azureuser supergroup          0 2021-01-27 12:21 /tweets/result/_SUCCESS\n",
      "-rw-r--r--   1 azureuser supergroup   10107405 2021-01-27 12:21 /tweets/result/part-00000\n",
      "-rw-r--r--   1 azureuser supergroup   10134121 2021-01-27 12:21 /tweets/result/part-00001\n",
      "-rw-r--r--   1 azureuser supergroup   10118293 2021-01-27 12:21 /tweets/result/part-00002\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tweets/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\n",
      "aaaaa\t7\n",
      "aaaaaaaaaaaaaa\t3\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\n",
      "aaaaaaaaaaaaaaaaaaah\t1\n",
      "aaaaaaaaaaaaand\t1\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\n",
      "aaaaaaaah\t1\n",
      "aaaaaaaamen\t1\n",
      "aaaaaaagh\t2\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /tweets/result/part-* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посмотреть на консоль хадупа можно\n",
    "* Добавить *.internal.cloudapp.net в прокси\n",
    "* Добавить headnodehost в прокси\n",
    "* Поднять прокси через головную ноду кластера\n",
    "* Открыть `http://headnodehost:19888/jobhistory`\n",
    "* Или открыть `http://hn1-hadoop.n3hsvtzmijuexf0fi4yqszkanb.bx.internal.cloudapp.net:8088/cluster` (ссылка может отличаться - она всегда пишется в консоли при запуске MR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим вторую задачу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tweets/top10/': No such file or directory\n",
      "WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\n",
      "packageJobJar: [] [/usr/hdp/4.1.2.5/hadoop/hadoop-streaming-3.1.3.4.1.2.5.jar] /tmp/streamjob2022702067033492020.jar tmpDir=null\n",
      "21/01/27 12:28:10 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:28:10 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:28:10 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:28:10 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:28:13 INFO client.RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\n",
      "21/01/27 12:28:13 INFO client.RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]\n",
      "21/01/27 12:28:14 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "21/01/27 12:28:15 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "21/01/27 12:28:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611730603892_0013\n",
      "21/01/27 12:28:16 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/01/27 12:28:17 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/4.1.2.5/0/resource-types.xml\n",
      "21/01/27 12:28:18 INFO impl.YarnClientImpl: Submitted application application_1611730603892_0013\n",
      "21/01/27 12:28:18 INFO mapreduce.Job: The url to track the job: http://hn1-lsml-h.bfsi4i1gtejurkfnybdpomj4ic.dx.internal.cloudapp.net:8088/proxy/application_1611730603892_0013/\n",
      "21/01/27 12:28:18 INFO mapreduce.Job: Running job: job_1611730603892_0013\n",
      "21/01/27 12:28:25 INFO mapreduce.Job: Job job_1611730603892_0013 running in uber mode : false\n",
      "21/01/27 12:28:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/27 12:28:35 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "21/01/27 12:28:39 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "21/01/27 12:28:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/01/27 12:28:48 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/27 12:28:49 INFO mapreduce.Job: Job job_1611730603892_0013 completed successfully\n",
      "21/01/27 12:28:50 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=38855035\n",
      "\t\tFILE: Number of bytes written=78709435\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tWASBS: Number of bytes read=30360731\n",
      "\t\tWASBS: Number of bytes written=106\n",
      "\t\tWASBS: Number of read operations=0\n",
      "\t\tWASBS: Number of large read operations=0\n",
      "\t\tWASBS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38748\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6993\n",
      "\t\tTotal time spent by all map tasks (ms)=38748\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6993\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=38748\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6993\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=119033856\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=21482496\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=38855047\n",
      "\t\tInput split bytes=456\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=38855047\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=1055\n",
      "\t\tCPU time spent (ms)=44480\n",
      "\t\tPhysical memory (bytes) snapshot=5601398784\n",
      "\t\tVirtual memory (bytes) snapshot=17951510528\n",
      "\t\tTotal committed heap usage (bytes)=10664017920\n",
      "\t\tPeak Map Physical memory (bytes)=1710878720\n",
      "\t\tPeak Map Virtual memory (bytes)=4479840256\n",
      "\t\tPeak Reduce Physical memory (bytes)=480686080\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4512710656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30359819\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=106\n",
      "21/01/27 12:28:50 INFO streaming.StreamJob: Output directory: /tweets/top10/\n",
      "CPU times: user 948 ms, sys: 236 ms, total: 1.18 s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /tweets/top10/\n",
    "! yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python top10.py map\" \\\n",
    "-reducer \"python top10.py reduce\" \\\n",
    "-input /tweets/result/ \\\n",
    "-output /tweets/top10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 azureuser supergroup          0 2021-01-27 12:28 /tweets/top10/_SUCCESS\n",
      "-rw-r--r--   1 azureuser supergroup        106 2021-01-27 12:28 /tweets/top10/part-00000\n",
      "21/01/27 12:28:59 WARN impl.MetricsSinkAdapter: azurefs2 has a full queue and can't consume the given metrics.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tweets/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\n",
      "co\t2833375\n",
      "https\t2454132\n",
      "the\t591885\n",
      "to\t589004\n",
      "in\t457433\n",
      "a\t412888\n",
      "s\t397889\n",
      "http\t375299\n",
      "of\t350983\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /tweets/top10/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed cache\n",
    "\n",
    "Помимо самого скрипта, мы можем положить в MapReduce любой другой файл, который может пригодиться для работы программы. Например при подсчете количества слов мы бы хотели выкинуть \"стоп-слова\". Их количество скорее всего не очень большое поэтому смело может передавать их обычным файлом. Hadoop гарантирует, что доставит все файлы ко всем машинам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /tweets/top10/part-* > stop-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Хозяйке на заметку\n",
    "\n",
    "В питоне уже есть хорошая стандартная библиотека, которая позволяет вам гораздо удобнее работать с такимим стримовыми данными. Давайте напишем новую задачу со стоп словами, чтобы они смотрелись поприличнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount2.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top10-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-2.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\r\n",
      "co\t2833375\r\n",
      "https\t2454132\r\n",
      "the\t591885\r\n",
      "to\t589004\r\n",
      "in\t457433\r\n",
      "a\t412888\r\n",
      "s\t397889\r\n",
      "http\t375299\r\n",
      "of\t350983\r\n"
     ]
    }
   ],
   "source": [
    "! cat stop-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tweets/top10-stop-words/': No such file or directory\n",
      "21/01/27 12:38:10 WARN impl.MetricsSinkAdapter: azurefs2 has a full queue and can't consume the given metrics.\n",
      "WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\n",
      "packageJobJar: [] [/usr/hdp/4.1.2.5/hadoop/hadoop-streaming-3.1.3.4.1.2.5.jar] /tmp/streamjob6652079870412143204.jar tmpDir=null\n",
      "21/01/27 12:38:20 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:38:20 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:38:20 ERROR sender.RawSocketSender: org.fluentd.logger.sender.RawSocketSender\n",
      "java.net.SocketTimeoutException\n",
      "\tat java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:111)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat org.fluentd.logger.sender.RawSocketSender.connect(RawSocketSender.java:85)\n",
      "\tat org.fluentd.logger.sender.RawSocketSender.reconnect(RawSocketSender.java:94)\n",
      "\tat org.fluentd.logger.sender.RawSocketSender.flush(RawSocketSender.java:193)\n",
      "\tat org.fluentd.logger.sender.RawSocketSender.send(RawSocketSender.java:184)\n",
      "\tat org.fluentd.logger.sender.RawSocketSender.emit(RawSocketSender.java:149)\n",
      "\tat org.fluentd.logger.sender.RawSocketSender.emit(RawSocketSender.java:131)\n",
      "\tat org.fluentd.logger.sender.RawSocketSender.emit(RawSocketSender.java:126)\n",
      "\tat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\n",
      "\tat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\n",
      "\tat com.microsoft.mdsdclient.MessageSendingRunnable$1.call(Unknown Source)\n",
      "\tat com.microsoft.mdsdclient.MessageSendingRunnable$1.call(Unknown Source)\n",
      "\tat com.microsoft.mdsdclient.RetryUtil.retry(Unknown Source)\n",
      "\tat com.microsoft.mdsdclient.RetryUtil.retry(Unknown Source)\n",
      "\tat com.microsoft.mdsdclient.MessageSendingRunnable.sendMessage(Unknown Source)\n",
      "\tat com.microsoft.mdsdclient.MessageSendingRunnable.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/01/27 12:38:21 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:38:21 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:38:24 INFO client.RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\n",
      "21/01/27 12:38:25 INFO client.RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]\n",
      "21/01/27 12:38:26 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "21/01/27 12:38:27 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "21/01/27 12:38:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611730603892_0014\n",
      "21/01/27 12:38:27 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/01/27 12:38:32 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/4.1.2.5/0/resource-types.xml\n",
      "21/01/27 12:38:32 INFO impl.YarnClientImpl: Submitted application application_1611730603892_0014\n",
      "21/01/27 12:38:33 INFO mapreduce.Job: The url to track the job: http://hn1-lsml-h.bfsi4i1gtejurkfnybdpomj4ic.dx.internal.cloudapp.net:8088/proxy/application_1611730603892_0014/\n",
      "21/01/27 12:38:33 INFO mapreduce.Job: Running job: job_1611730603892_0014\n",
      "21/01/27 12:38:39 INFO mapreduce.Job: Job job_1611730603892_0014 running in uber mode : false\n",
      "21/01/27 12:38:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/27 12:38:50 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "21/01/27 12:38:54 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "21/01/27 12:38:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/01/27 12:39:04 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "21/01/27 12:39:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/27 12:39:07 INFO mapreduce.Job: Job job_1611730603892_0014 completed successfully\n",
      "21/01/27 12:39:08 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=38855035\n",
      "\t\tFILE: Number of bytes written=78710711\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tWASBS: Number of bytes read=30360731\n",
      "\t\tWASBS: Number of bytes written=109\n",
      "\t\tWASBS: Number of read operations=0\n",
      "\t\tWASBS: Number of large read operations=0\n",
      "\t\tWASBS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40652\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9663\n",
      "\t\tTotal time spent by all map tasks (ms)=40652\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9663\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=40652\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9663\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=124882944\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=29684736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=38855047\n",
      "\t\tInput split bytes=456\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=38855047\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=877\n",
      "\t\tCPU time spent (ms)=47240\n",
      "\t\tPhysical memory (bytes) snapshot=5596925952\n",
      "\t\tVirtual memory (bytes) snapshot=17949372416\n",
      "\t\tTotal committed heap usage (bytes)=10664542208\n",
      "\t\tPeak Map Physical memory (bytes)=1711214592\n",
      "\t\tPeak Map Virtual memory (bytes)=4480012288\n",
      "\t\tPeak Reduce Physical memory (bytes)=1132568576\n",
      "\t\tPeak Reduce Virtual memory (bytes)=5183840256\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30359819\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "21/01/27 12:39:08 INFO streaming.StreamJob: Output directory: /tweets/top10-stop-words/\n",
      "CPU times: user 1.19 s, sys: 230 ms, total: 1.42 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /tweets/top10-stop-words/\n",
    "! yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10-stop-words\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10-2.py,stop-words.txt \\\n",
    "-mapper \"python top10-2.py map\" \\\n",
    "-reducer \"python top10-2.py reduce\" \\\n",
    "-input /tweets/result/ \\\n",
    "-output /tweets/top10-stop-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /tweets/top10-stop-words/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ускоряем вычисления \n",
    "\n",
    "Несмотря на все оптимизации внутри Hadoop, самое узкое место - это передача данных от mapper к reducer. Таким образом если у нас получиться ускорить выполнение ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount3.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    counter = Counter()\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "    \n",
    "    for word, number in counter.items():\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tweets/result-fast1': No such file or directory\n",
      "WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\n",
      "packageJobJar: [] [/usr/hdp/4.1.2.5/hadoop/hadoop-streaming-3.1.3.4.1.2.5.jar] /tmp/streamjob3276516031445984988.jar tmpDir=null\n",
      "21/01/27 12:47:50 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:47:50 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:47:50 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:47:50 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:47:52 INFO client.RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\n",
      "21/01/27 12:47:53 INFO client.RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]\n",
      "21/01/27 12:47:54 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "21/01/27 12:47:54 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "21/01/27 12:47:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611730603892_0015\n",
      "21/01/27 12:47:54 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/01/27 12:47:55 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/4.1.2.5/0/resource-types.xml\n",
      "21/01/27 12:47:56 INFO impl.YarnClientImpl: Submitted application application_1611730603892_0015\n",
      "21/01/27 12:47:56 INFO mapreduce.Job: The url to track the job: http://hn1-lsml-h.bfsi4i1gtejurkfnybdpomj4ic.dx.internal.cloudapp.net:8088/proxy/application_1611730603892_0015/\n",
      "21/01/27 12:47:56 INFO mapreduce.Job: Running job: job_1611730603892_0015\n",
      "21/01/27 12:48:02 INFO mapreduce.Job: Job job_1611730603892_0015 running in uber mode : false\n",
      "21/01/27 12:48:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/27 12:48:15 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "21/01/27 12:48:16 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "21/01/27 12:48:18 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "21/01/27 12:48:19 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "21/01/27 12:48:21 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "21/01/27 12:48:22 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "21/01/27 12:48:23 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "21/01/27 12:48:24 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "21/01/27 12:48:25 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "21/01/27 12:48:27 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "21/01/27 12:48:28 INFO mapreduce.Job:  map 82% reduce 0%\n",
      "21/01/27 12:48:30 INFO mapreduce.Job:  map 87% reduce 0%\n",
      "21/01/27 12:48:31 INFO mapreduce.Job:  map 92% reduce 21%\n",
      "21/01/27 12:48:32 INFO mapreduce.Job:  map 100% reduce 21%\n",
      "21/01/27 12:48:33 INFO mapreduce.Job:  map 100% reduce 37%\n",
      "21/01/27 12:48:34 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "21/01/27 12:48:36 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "21/01/27 12:48:37 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "21/01/27 12:48:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/27 12:48:39 INFO mapreduce.Job: Job job_1611730603892_0015 completed successfully\n",
      "21/01/27 12:48:40 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=53133205\n",
      "\t\tFILE: Number of bytes written=110255904\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tWASBS: Number of bytes read=1140713466\n",
      "\t\tWASBS: Number of bytes written=30359819\n",
      "\t\tWASBS: Number of read operations=0\n",
      "\t\tWASBS: Number of large read operations=0\n",
      "\t\tWASBS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tRack-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=275767\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=51349\n",
      "\t\tTotal time spent by all map tasks (ms)=275767\n",
      "\t\tTotal time spent by all reduce tasks (ms)=51349\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=275767\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=51349\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=847156224\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=157744128\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=4292696\n",
      "\t\tMap output bytes=44547794\n",
      "\t\tMap output materialized bytes=53133421\n",
      "\t\tInput split bytes=2110\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=53133421\n",
      "\t\tReduce input records=4292696\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=8585392\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=2708\n",
      "\t\tCPU time spent (ms)=167580\n",
      "\t\tPhysical memory (bytes) snapshot=22858399744\n",
      "\t\tVirtual memory (bytes) snapshot=71778177024\n",
      "\t\tTotal committed heap usage (bytes)=42354081792\n",
      "\t\tPeak Map Physical memory (bytes)=1767878656\n",
      "\t\tPeak Map Virtual memory (bytes)=4606222336\n",
      "\t\tPeak Reduce Physical memory (bytes)=391077888\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4543148032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1140698687\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "21/01/27 12:48:40 INFO streaming.StreamJob: Output directory: /tweets/result-fast1/\n",
      "CPU times: user 1.29 s, sys: 274 ms, total: 1.57 s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /tweets/result-fast1 || true\n",
    "! yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount3.py \\\n",
    "-mapper \"python3 wordcount3.py map\" \\\n",
    "-reducer \"python3 wordcount3.py reduce\" \\\n",
    "-input /tweets/data/ \\\n",
    "-output /tweets/result-fast1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\n",
      "aaaaa\t7\n",
      "aaaaaaaaaaaaaa\t3\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\n",
      "aaaaaaaaaaaaaaaaaaah\t1\n",
      "aaaaaaaaaaaaand\t1\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\n",
      "aaaaaaaah\t1\n",
      "aaaaaaaamen\t1\n",
      "aaaaaaagh\t2\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у этого решения есть **очень большой минус** - сложность по памяти **O(n)**. Это означает, что вычисление может упасть если данные попадутся неудачные. \n",
    "\n",
    "Важный принцип работы с большими данными - все алгоритмы должны работать меньше чем за O(n). Это относится не только к MapReduce, а в целом почти к любым инструментам обработки больших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем комбайнер\n",
    "\n",
    "Чтобы побороться с этой бедой, воспользуемся дополнительным инструментом в Hadoop - Combiner. По сути это маленький Reduce, который запускается после маппера. Это позволяет уменьшить количество выходных данных с Map стадии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/getpro/habr/post_images/587/2d2/dfe/5872d2dfe12643665370708d225bc1d4.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tweets/result-fast2': No such file or directory\n",
      "WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\n",
      "packageJobJar: [] [/usr/hdp/4.1.2.5/hadoop/hadoop-streaming-3.1.3.4.1.2.5.jar] /tmp/streamjob8321391744385464559.jar tmpDir=null\n",
      "21/01/27 12:55:34 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:55:34 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:55:34 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 12:55:34 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 12:55:36 INFO client.RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\n",
      "21/01/27 12:55:36 INFO client.RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]\n",
      "21/01/27 12:55:38 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "21/01/27 12:55:38 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "21/01/27 12:55:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611730603892_0016\n",
      "21/01/27 12:55:38 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/01/27 12:55:39 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/4.1.2.5/0/resource-types.xml\n",
      "21/01/27 12:55:40 INFO impl.YarnClientImpl: Submitted application application_1611730603892_0016\n",
      "21/01/27 12:55:40 INFO mapreduce.Job: The url to track the job: http://hn1-lsml-h.bfsi4i1gtejurkfnybdpomj4ic.dx.internal.cloudapp.net:8088/proxy/application_1611730603892_0016/\n",
      "21/01/27 12:55:40 INFO mapreduce.Job: Running job: job_1611730603892_0016\n",
      "21/01/27 12:55:47 INFO mapreduce.Job: Job job_1611730603892_0016 running in uber mode : false\n",
      "21/01/27 12:55:47 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/27 12:55:59 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "21/01/27 12:56:00 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "21/01/27 12:56:02 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "21/01/27 12:56:03 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "21/01/27 12:56:05 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "21/01/27 12:56:06 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "21/01/27 12:56:08 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "21/01/27 12:56:09 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "21/01/27 12:56:11 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "21/01/27 12:56:12 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "21/01/27 12:56:14 INFO mapreduce.Job:  map 69% reduce 2%\n",
      "21/01/27 12:56:15 INFO mapreduce.Job:  map 69% reduce 3%\n",
      "21/01/27 12:56:16 INFO mapreduce.Job:  map 72% reduce 3%\n",
      "21/01/27 12:56:17 INFO mapreduce.Job:  map 74% reduce 3%\n",
      "21/01/27 12:56:18 INFO mapreduce.Job:  map 74% reduce 5%\n",
      "21/01/27 12:56:19 INFO mapreduce.Job:  map 77% reduce 5%\n",
      "21/01/27 12:56:20 INFO mapreduce.Job:  map 87% reduce 10%\n",
      "21/01/27 12:56:21 INFO mapreduce.Job:  map 90% reduce 12%\n",
      "21/01/27 12:56:22 INFO mapreduce.Job:  map 97% reduce 12%\n",
      "21/01/27 12:56:23 INFO mapreduce.Job:  map 100% reduce 37%\n",
      "21/01/27 12:56:24 INFO mapreduce.Job:  map 100% reduce 55%\n",
      "21/01/27 12:56:26 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "21/01/27 12:56:27 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "21/01/27 12:56:28 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/27 12:56:29 INFO mapreduce.Job: Job job_1611730603892_0016 completed successfully\n",
      "21/01/27 12:56:30 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=53133205\n",
      "\t\tFILE: Number of bytes written=110261872\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tWASBS: Number of bytes read=1140713466\n",
      "\t\tWASBS: Number of bytes written=30359819\n",
      "\t\tWASBS: Number of read operations=0\n",
      "\t\tWASBS: Number of large read operations=0\n",
      "\t\tWASBS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tRack-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=379995\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=70025\n",
      "\t\tTotal time spent by all map tasks (ms)=379995\n",
      "\t\tTotal time spent by all reduce tasks (ms)=70025\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=379995\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=70025\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1167344640\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=215116800\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=53133421\n",
      "\t\tInput split bytes=2110\n",
      "\t\tCombine input records=41946754\n",
      "\t\tCombine output records=4292696\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=53133421\n",
      "\t\tReduce input records=4292696\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=8585392\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=2614\n",
      "\t\tCPU time spent (ms)=249790\n",
      "\t\tPhysical memory (bytes) snapshot=22821744640\n",
      "\t\tVirtual memory (bytes) snapshot=71771086848\n",
      "\t\tTotal committed heap usage (bytes)=42354081792\n",
      "\t\tPeak Map Physical memory (bytes)=1688743936\n",
      "\t\tPeak Map Virtual memory (bytes)=4515635200\n",
      "\t\tPeak Reduce Physical memory (bytes)=390549504\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4542873600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1140698687\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "21/01/27 12:56:30 INFO streaming.StreamJob: Output directory: /tweets/result-fast2/\n",
      "CPU times: user 1.15 s, sys: 271 ms, total: 1.42 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /tweets/result-fast2 || true\n",
    "! yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount2.py \\\n",
    "-mapper \"python3 wordcount2.py map\" \\\n",
    "-combiner \"python3 wordcount2.py reduce\" \\\n",
    "-reducer \"python3 wordcount2.py reduce\" \\\n",
    "-input /tweets/data/ \\\n",
    "-output /tweets/result-fast2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\n",
      "aaaaa\t7\n",
      "aaaaaaaaaaaaaa\t3\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\n",
      "aaaaaaaaaaaaaaaaaaah\t1\n",
      "aaaaaaaaaaaaand\t1\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\n",
      "aaaaaaaah\t1\n",
      "aaaaaaaamen\t1\n",
      "aaaaaaagh\t2\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто combiner может просто совпадать с reducer однако это не всегда так по следующей причине - combiner не имеет права менять формат вывода после стадии map.\n",
    "\n",
    "Hadoop самостоятельно опеределяет целесообразность запуска combiner и может его не запускать вовсе.\n",
    "Или например задача может вообще не подходить под такую модель запуска. Если мы ищем среднее, то нельзя заранее подсчитывать среднее на стадии combiner - макмимум, что мы там можем запустить - это подсчет количество и суммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top10-3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-3.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "    \n",
    "def combiner():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}+{}\\t\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer,\n",
    "        'combiner': combiner\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tweets/top10-fast': No such file or directory\n",
      "WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\n",
      "packageJobJar: [] [/usr/hdp/4.1.2.5/hadoop/hadoop-streaming-3.1.3.4.1.2.5.jar] /tmp/streamjob6007250661757710875.jar tmpDir=null\n",
      "21/01/27 13:04:07 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 13:04:07 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 13:04:07 INFO client.RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\n",
      "21/01/27 13:04:07 INFO client.AHSProxy: Connecting to Application History server at headnodehost/10.0.0.21:10200\n",
      "21/01/27 13:04:09 INFO client.RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\n",
      "21/01/27 13:04:09 INFO client.RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]\n",
      "21/01/27 13:04:10 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "21/01/27 13:04:11 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "21/01/27 13:04:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611730603892_0017\n",
      "21/01/27 13:04:11 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "21/01/27 13:04:12 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/4.1.2.5/0/resource-types.xml\n",
      "21/01/27 13:04:13 INFO impl.YarnClientImpl: Submitted application application_1611730603892_0017\n",
      "21/01/27 13:04:13 INFO mapreduce.Job: The url to track the job: http://hn1-lsml-h.bfsi4i1gtejurkfnybdpomj4ic.dx.internal.cloudapp.net:8088/proxy/application_1611730603892_0017/\n",
      "21/01/27 13:04:13 INFO mapreduce.Job: Running job: job_1611730603892_0017\n",
      "21/01/27 13:04:19 INFO mapreduce.Job: Job job_1611730603892_0017 running in uber mode : false\n",
      "21/01/27 13:04:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/27 13:04:29 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "21/01/27 13:04:30 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "21/01/27 13:04:34 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "21/01/27 13:04:35 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "21/01/27 13:04:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/01/27 13:04:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/27 13:04:40 INFO mapreduce.Job: Job job_1611730603892_0017 completed successfully\n",
      "21/01/27 13:04:40 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=395\n",
      "\t\tFILE: Number of bytes written=1002919\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tWASBS: Number of bytes read=30360767\n",
      "\t\tWASBS: Number of bytes written=109\n",
      "\t\tWASBS: Number of read operations=0\n",
      "\t\tWASBS: Number of large read operations=0\n",
      "\t\tWASBS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38911\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3104\n",
      "\t\tTotal time spent by all map tasks (ms)=38911\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3104\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=38911\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3104\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=119534592\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9535488\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=407\n",
      "\t\tInput split bytes=474\n",
      "\t\tCombine input records=2831736\n",
      "\t\tCombine output records=30\n",
      "\t\tReduce input groups=30\n",
      "\t\tReduce shuffle bytes=407\n",
      "\t\tReduce input records=30\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=60\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=859\n",
      "\t\tCPU time spent (ms)=39860\n",
      "\t\tPhysical memory (bytes) snapshot=5435396096\n",
      "\t\tVirtual memory (bytes) snapshot=17950318592\n",
      "\t\tTotal committed heap usage (bytes)=10664017920\n",
      "\t\tPeak Map Physical memory (bytes)=1710923776\n",
      "\t\tPeak Map Virtual memory (bytes)=4480339968\n",
      "\t\tPeak Reduce Physical memory (bytes)=316809216\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4510961664\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30359819\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "21/01/27 13:04:40 INFO streaming.StreamJob: Output directory: /tweets/top10-fast/\n",
      "CPU times: user 812 ms, sys: 214 ms, total: 1.03 s\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /tweets/top10-fast || true\n",
    "! yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files ~/top10-3.py,stop-words.txt \\\n",
    "-mapper \"python3 top10-3.py map\" \\\n",
    "-combiner \"python3 top10-3.py combiner\" \\\n",
    "-reducer \"python3 top10-3.py reduce\" \\\n",
    "-input /tweets/result-fast1 \\\n",
    "-output /tweets/top10-fast/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /tweets/top10-fast/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Самостоятельное упражнение для искушенного слушателя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь мы попробуем обработать скачанные выше данные по-взрослому. Пусть у нас есть какой-то SQL движок (здесь и далее будет использоваться синтаксис Clickhouse), в котором есть вот такая таблица:\n",
    "\n",
    "```sql\n",
    "create table tweet_data (\n",
    "    external_author_id   String,\n",
    "    author               String,\n",
    "    content              String,\n",
    "    region               String,\n",
    "    language             String,\n",
    "    publish_date         String,\n",
    "    harvested_date       String,\n",
    "    following            String,\n",
    "    followers            String,\n",
    "    updates              String,\n",
    "    post_type            String,\n",
    "    account_type         String,\n",
    "    retweet              String,\n",
    "    account_category     String,\n",
    "    new_june_2018        String,\n",
    "    alt_external_id      String,\n",
    "    tweet_id             String,\n",
    "    article_url          String,\n",
    "    tco1_step1           String,\n",
    "    tco2_step1           String,\n",
    "    tco3_step1           String\n",
    ")\n",
    "engine = MergeTree()\n",
    "    order by author\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для особых ценителей:\n",
    "```bash\n",
    "python -c 'import csv; data=open(\"tweets_1.csv\", newline=\"\"); print(\"\\n\".join(map(lambda line: \"|||\".join(map(lambda rawline: rawline.replace(\"\\t\", \"\"), line)), csv.reader(data))));' \\\n",
    "    | tail -n+2 \\\n",
    "    | sed 's:\\\\:\\\\\\\\:g' \\\n",
    "    | sed 's:\\t: :g' \\\n",
    "    | sed 's:|||:\\t:g' \\\n",
    "    | clickhouse local \\\n",
    "        --input-format TSV \\\n",
    "        --table tmp \\\n",
    "        --structure 'external_author_id String, author String, content String, region String, language String, publish_date String, harvested_date String, following String, followers String, updates String, post_type String, account_type String, retweet String, account_category String, new_june_2018 String, alt_external_id String, tweet_id String, article_url String, tco1_step1 String, tco2_step1 String, tco3_step1 String'\\\n",
    "        --query \"select * from tmp limit 1\"\n",
    "```\n",
    "\n",
    "Чтобы было удобнее работать с этими данным ad-hoc лучше всего предобработать данные и создать временный `alias` в каком-то терминале:\n",
    "\n",
    "```bash\n",
    "python -c 'import csv; data=open(\"tweets_1.csv\", newline=\"\"); print(\"\\n\".join(map(lambda line: \"|||\".join(map(lambda rawline: rawline.replace(\"\\t\", \"\"), line)), csv.reader(data))));' \\\n",
    "    | tail -n+2 \\\n",
    "    | sed 's:\\\\:\\\\\\\\:g' \\\n",
    "    | sed 's:\\t: :g' \\\n",
    "    | sed 's:|||:\\t:g' > tweets_1_treated.csv\n",
    "    \n",
    "alias tmpch=\"cat tweets_1_treated.csv \\\n",
    "    | clickhouse local \\\n",
    "        --input-format TSV \\\n",
    "        --table tmp \\\n",
    "        --structure 'external_author_id String, author String, content String, region String, language String, publish_date String, harvested_date String, following String, followers String, updates String, post_type String, account_type String, retweet String, account_category String, new_june_2018 String, alt_external_id String, tweet_id String, article_url String, tco1_step1 String, tco2_step1 String, tco3_step1 String'\\\n",
    "        --query\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К нам поступает задача: посчитать топ-10 пользователей, которые написали больше 100 твитов в разбивке по регионам. Примерное решение задачи может выглядеть следующим образом:\n",
    "\n",
    "\n",
    "```sql\n",
    "select region, \n",
    "       sum(num_tweets) as num_tweets, \n",
    "       arrayStringConcat(arraySlice(groupArray(author), 1, 10), ', ') \n",
    "from (\n",
    "    select author, \n",
    "           region, \n",
    "           count() as num_tweets\n",
    "    from tmp\n",
    "    group by author, region\n",
    "    having num_tweets > 100\n",
    "    order by num_tweets DESC\n",
    ") \n",
    "group by region \n",
    "order by num_tweets DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на стадии выполнения запроса:\n",
    "1. Выбираем все пары пользователь-регион, у которых сумма твитов > 100\n",
    "2. Сортируем в порядке убывания количества твитов\n",
    "3. Группируем по региону, считаем сумму пользователей\n",
    "4. Для каждого региона берем 10 первых авторов и перечисляем их ники через запятую\n",
    "\n",
    "Подумайте, как бы вы это реализовали на map-reduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой шаг здесь лучше всего?\n",
    "\n",
    "1. Здесь будет следующая последовательность map-reduce-map-reduce: шаг map прочитает файл и отфильтрует ненужные колонки, шаг reduce -- посчитает по каждой паре пользователь-региону сумму твитов, map отфильтрует среди них те, у которых твитов больше 100 и переформатирует данные так, что они приумт вид `f\"{num_tweets} {author} {region}\"`, а последняя функция reduce будет identity-фнукцией\n",
    "2. Этот шаг мы можем не делать -- на выходе из предыдущего шага ключ-значение уже дадут нам нужную сортировку\n",
    "3. Здесь будет map-reduce: шаг map преобразует файл в `f\"{region} {author} {num_tweets}\"`, затем reduce соберет по каждому региону суммарную статистику и выдаст в результате файл с регионом, числом твитов и пользователями, где каждая строка -- свой пользователь\n",
    "4. Здесь будет map-reduce: шаг map будет ничего не делать, а шаг reduce соберет нужную нам строчку для каждого региона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение задачи может выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T22:17:18.500597Z",
     "start_time": "2021-01-25T22:17:18.489562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper-projection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-projection.py\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "def main():\n",
    "    for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "        current_row = [f\"{row[1]}_{[3]}\", 1]\n",
    "        print(\"\\t\".join(current_row))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T22:22:10.083803Z",
     "start_time": "2021-01-25T22:22:10.079519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer-groupby-sum.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-groupby-sum.py\n",
    "import collections\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "def main():\n",
    "    groupby = collections.defaultdict(int)\n",
    "    \n",
    "    for (author_region, tweet_count) in csv.reader(iter(sys.stdin.readline, ''), delimiter='\\t'):\n",
    "        tweet_count = int(tweet_count)\n",
    "        groupby[author_region] += tweet_count\n",
    "        \n",
    "    for (author_region, tweet_count) in groupby:\n",
    "        print(f\"{author_region}\\t{tweet_count}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T22:23:09.180061Z",
     "start_time": "2021-01-25T22:23:09.176292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper-filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-filter.py\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "def main():\n",
    "    for (author_region, tweet_count) in csv.reader(iter(sys.stdin.readline, ''), delimiter='\\t'):\n",
    "        author, region = author_region.split(\"_\")\n",
    "        if int(tweet_count) > 100:\n",
    "            print(f\"{tweet_count}\\t{author}\\t{region}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T22:24:13.308854Z",
     "start_time": "2021-01-25T22:24:13.304532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing identity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile identity.py\n",
    "def main():\n",
    "    for line in iter(sys.stdin.readline, ''):\n",
    "        print(line)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T22:24:40.449394Z",
     "start_time": "2021-01-25T22:24:40.446942Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO!: допишите код так, чтобы он заработал"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop жжет бабло\n",
    "\n",
    "<img src=\"http://vostokovod.ru/assets/images/blog/2013/000333.png\">\n",
    "\n",
    "Полноценный кластер - весьма дорогое удовольствие, поэтому отлючайте его после использования. \n",
    "\n",
    "**Если в HDFS у вас лежит какой-то результат, который вы не хотите терять - не удаляйте `storage container`!**. К новому кластеру можно будет подключить сохраненный контейнер и продолжить работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An execution plan has been generated and is shown below.\n",
      "Resource actions are indicated with the following symbols:\n",
      "  \u001b[31m-\u001b[0m destroy\n",
      "\u001b[0m\n",
      "Terraform will perform the following actions:\n",
      "\n",
      "\u001b[1m  # azurerm_hdinsight_hadoop_cluster.lsml_hc\u001b[0m will be \u001b[1m\u001b[31mdestroyed\u001b[0m\u001b[0m\n",
      "\u001b[0m  \u001b[31m-\u001b[0m\u001b[0m resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\" {\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mcluster_version\u001b[0m\u001b[0m     = \"4.0.2000.1\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mhttps_endpoint\u001b[0m\u001b[0m      = \"lsml-hdicluster.azurehdinsight.net\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m                  = \"/subscriptions/7d1225ca-27cc-40b7-8036-c62a48072ba8/resourceGroups/lsml-resource-group/providers/Microsoft.HDInsight/clusters/lsml-hdicluster\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mlocation\u001b[0m\u001b[0m            = \"westus\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mname\u001b[0m\u001b[0m                = \"lsml-hdicluster\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mresource_group_name\u001b[0m\u001b[0m = \"lsml-resource-group\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_endpoint\u001b[0m\u001b[0m        = \"lsml-hdicluster-ssh.azurehdinsight.net\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mtags\u001b[0m\u001b[0m                = {} \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mtier\u001b[0m\u001b[0m                = \"standard\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "\n",
      "      \u001b[31m-\u001b[0m \u001b[0mcomponent_version {\n",
      "          \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mhadoop\u001b[0m\u001b[0m = \"3.1\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "        }\n",
      "\n",
      "      \u001b[31m-\u001b[0m \u001b[0mgateway {\n",
      "          \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0menabled\u001b[0m\u001b[0m  = true \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "          \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "          \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "        }\n",
      "\n",
      "      \u001b[31m-\u001b[0m \u001b[0mroles {\n",
      "\n",
      "          \u001b[31m-\u001b[0m \u001b[0mhead_node {\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_keys\u001b[0m\u001b[0m = [] \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m  = \"A5\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "            }\n",
      "\n",
      "          \u001b[31m-\u001b[0m \u001b[0mworker_node {\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mmin_instance_count\u001b[0m\u001b[0m    = 0 \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m              = (sensitive value)\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_keys\u001b[0m\u001b[0m              = [] \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mtarget_instance_count\u001b[0m\u001b[0m = 2 \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m              = \"azureuser\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m               = \"Standard_D12_V2\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "            }\n",
      "\n",
      "          \u001b[31m-\u001b[0m \u001b[0mzookeeper_node {\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_keys\u001b[0m\u001b[0m = [] \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "              \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m  = \"Standard_A2_V2\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "            }\n",
      "        }\n",
      "\n",
      "      \u001b[31m-\u001b[0m \u001b[0mstorage_account {\n",
      "          \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mis_default\u001b[0m\u001b[0m           = true \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "          \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mstorage_account_key\u001b[0m\u001b[0m  = (sensitive value)\n",
      "          \u001b[31m-\u001b[0m \u001b[0m\u001b[1m\u001b[0mstorage_container_id\u001b[0m\u001b[0m = \"https://lsmlhdinsightstore.blob.core.windows.net/lsmlhdinsight\" \u001b[90m->\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m\n",
      "        }\n",
      "    }\n",
      "\n",
      "\u001b[0m\u001b[1mPlan:\u001b[0m 0 to add, 0 to change, 1 to destroy.\u001b[0m\n",
      "\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1mResource targeting is in effect\u001b[0m\n",
      "\n",
      "\u001b[0mYou are creating a plan with the -target option, which means that the result\n",
      "of this plan may not represent all of the changes requested by the current\n",
      "configuration.\n",
      "\t\t\n",
      "The -target option is not for routine use, and is provided only for\n",
      "exceptional situations such as recovering from errors or mistakes, or when\n",
      "Terraform specifically suggests to use it as part of an error message.\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1mVersion constraints inside provider configuration blocks are deprecated\u001b[0m\n",
      "\n",
      "\u001b[0m  on common.tf line 3, in provider \"azurerm\":\n",
      "   3:   version = \u001b[4m\"=2.40.0\"\u001b[0m\n",
      "\u001b[0m\n",
      "Terraform 0.13 and earlier allowed provider version constraints inside the\n",
      "provider configuration block, but that is now deprecated and will be removed\n",
      "in a future version of Terraform. To silence this warning, move the provider\n",
      "version constraint into the required_providers block.\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1m\"gateway.0.enabled\": [DEPRECATED] HDInsight doesn't support disabling gateway anymore\u001b[0m\n",
      "\n",
      "\u001b[0m  on hadoop.tf line 8, in resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\":\n",
      "   8: resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\" \u001b[4m{\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mDo you really want to destroy all resources?\u001b[0m\n",
      "  Terraform will destroy all your managed infrastructure, as shown above.\n",
      "  There is no undo. Only 'yes' will be accepted to confirm.\n",
      "\n",
      "  \u001b[1mEnter a value:\u001b[0m \u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Destroying... [id=/subscriptions/7d1225ca-27cc-40b7-8036-c62a48072ba8/resourceGroups/lsml-resource-group/providers/Microsoft.HDInsight/clusters/lsml-hdicluster]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still destroying... [id=/subscriptions/7d1225ca-27cc-40b7-8036-...oft.HDInsight/clusters/lsml-hdicluster, 10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still destroying... [id=/subscriptions/7d1225ca-27cc-40b7-8036-...oft.HDInsight/clusters/lsml-hdicluster, 20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still destroying... [id=/subscriptions/7d1225ca-27cc-40b7-8036-...oft.HDInsight/clusters/lsml-hdicluster, 30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still destroying... [id=/subscriptions/7d1225ca-27cc-40b7-8036-...oft.HDInsight/clusters/lsml-hdicluster, 40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still destroying... [id=/subscriptions/7d1225ca-27cc-40b7-8036-...oft.HDInsight/clusters/lsml-hdicluster, 50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still destroying... [id=/subscriptions/7d1225ca-27cc-40b7-8036-...oft.HDInsight/clusters/lsml-hdicluster, 1m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Destruction complete after 1m4s\u001b[0m\u001b[0m\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1mApplied changes may be incomplete\u001b[0m\n",
      "\n",
      "\u001b[0mThe plan was created with the -target option in effect, so some changes\n",
      "requested in the configuration may have been ignored and the output values may\n",
      "not be fully updated. Run the following command to verify that no other\n",
      "changes are pending:\n",
      "    terraform plan\n",
      "\t\n",
      "Note that the -target option is not suitable for routine use, and is provided\n",
      "only for exceptional situations such as recovering from errors or mistakes, or\n",
      "when Terraform specifically suggests to use it as part of an error message.\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Destroy complete! Resources: 1 destroyed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Удаляем ТОЛЬКО кластер\n",
    "! echo \"yes\" | terraform destroy -target azurerm_hdinsight_hadoop_cluster.lsml_hc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
