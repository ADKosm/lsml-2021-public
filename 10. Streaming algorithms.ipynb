{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы на потоках данных\n",
    "\n",
    "На этом семинаре посмотрим на то, что можно сделать с большими данными, когда наши вычислительные возможности несколько ограничены. \n",
    "\n",
    "Ограниченость может быть вызвана различными факторами\n",
    "* У нас просто нет вычислительных ресурсов для hadoop кластера - есть только одна тачка для вычислений с жесткий диском\n",
    "* У нас все таки есть хадуп кластер, но при этом некоторые задачи все равно решаются на нем мучительно долго\n",
    "* У нас есть хадуп кластер, однако данные в огромных количествах прилетают каждую секунду (например сообщения из кафки или логи веб-серверов)\n",
    "* У нас есть только \"умная\" кофеварка и тонны данных для анализа - например мы строим Internet-of-things и хотим встроить какую-то аналитику в систему. В таком раскладе у нас есть небольшое устройство, которое подключено в гигантской сети из таких же устройств, каждое из которых непрерывно шлет показания с датчиков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не любую задачу можно решить за приемлимое время с приемлимым качеством в таких условиях, однако существует целый класс алгоритмов, которые умеют выдавать разумные результаты в подобых сценариях - стриминговые алгоритмы.\n",
    "\n",
    "Основные отличия стриминговых алгоритмов следующие:\n",
    "* Память у алгоритма ограничена и много меньше размера входных данных\n",
    "* Алгоритм может посмотреть на данные только 1 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет на сегодняшний семинар - данные с сенсоров \"умного\" города в Денмарке. Сенсоры снимают показания о разруженности дорог по городу и сливают их в единый поток данных.\n",
    "\n",
    "Информация по датасету - http://iot.ee.surrey.ac.uk:8080/datasets.html#traffic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-17 08:36:21--  http://iot.ee.surrey.ac.uk:8080/datasets/traffic/traffic_oct_nov/citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip\n",
      "Resolving iot.ee.surrey.ac.uk (iot.ee.surrey.ac.uk)... 131.227.92.114\n",
      "Connecting to iot.ee.surrey.ac.uk (iot.ee.surrey.ac.uk)|131.227.92.114|:8080... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 38520470 (37M) [application/zip]\n",
      "Saving to: ‘citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip.2’\n",
      "\n",
      "citypulse_traffic_r 100%[===================>]  36.74M  16.5MB/s    in 2.2s    \n",
      "\n",
      "2021-03-17 08:36:23 (16.5 MB/s) - ‘citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip.2’ saved [38520470/38520470]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://iot.ee.surrey.ac.uk:8080/datasets/traffic/traffic_oct_nov/citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip -d dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i '1d' dataset/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat dataset/* > traffic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4382599\r\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK,78,47,668,78,2014-10-01T01:45:00,0,28072438,158324\r",
      "\r\n",
      "OK,76,48,668,76,2014-10-01T01:50:00,1,28072887,158324\r",
      "\r\n",
      "OK,76,48,668,76,2014-10-01T01:55:00,1,28073334,158324\r",
      "\r\n",
      "OK,60,61,668,60,2014-10-01T02:05:00,0,28074100,158324\r",
      "\r\n",
      "OK,60,61,668,60,2014-10-01T02:10:00,0,28074549,158324\r",
      "\r\n",
      "OK,60,61,668,60,2014-10-01T02:15:00,0,28074989,158324\r",
      "\r\n",
      "OK,60,61,668,60,2014-10-01T02:20:00,0,28075438,158324\r",
      "\r\n",
      "OK,64,57,668,64,2014-10-01T02:25:00,1,28075887,158324\r",
      "\r\n",
      "OK,59,62,668,59,2014-10-01T02:30:00,2,28076270,158324\r",
      "\r\n",
      "OK,60,61,668,60,2014-10-01T02:35:00,4,28076719,158324\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head traffic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонки датасета следующие\n",
    "```\n",
    "status\tavgMeasuredTime\tavgSpeed\textID\tmedianMeasuredTime\tTIMESTAMP\tvehicleCount\t_id\tREPORT_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простые статистики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среднее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из самых простых стриминговых алгоритмов, который писал скорее всего каждый - это подсчет среднего значения набора чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mean-stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mean-stream.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "stream = map(lambda x: int(x[6]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "vehicle_count = 0\n",
    "record_count = 0\n",
    "\n",
    "for current_vehicle_count in stream:\n",
    "    vehicle_count += current_vehicle_count\n",
    "    record_count += 1\n",
    "\n",
    "print(vehicle_count / record_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что алгоритм использует O(1) памяти (бкувально две переменные) и проходится по всем данных ровно один раз. \n",
    "Это практически эталонный пример того, как структурно выглядит стриминговый алгоритм и далее мы будем говорить именно о подобных алгоритмах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:06<00:00, 724325.89it/s]\n",
      "3.0282777411303203\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 mean-stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что мы решили эту задачу точно (не приближенно). Аналогично мы можем посчитать и другие несложные статистики - количество, минимум, максимум, дисперсию и так далее.\n",
    "\n",
    "Однако не все статистики считаются с такой легкостью. Например есть большие проблемы с подсчетом медианы в один проход. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сложные статистики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Медиана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже предлагается к ознакомлению алгоритм для поиска медианы (Мунро-Патерсон).\n",
    "\n",
    "Идея крайне простая - возьмем T первых элементов из потока. Далее для всех следующих элементов будем подсчитывать, сколько из этих элементов больше (по значению), чем элементы из нашего множества и сколько элементов меньше. Если в конце окажется так, что и тех и тех (больших и меньших) элементов меньше, чем половина всех элементов в потоке (< N/2), то это означает, что наше множество содержит элементы как раз из середины упорядоченного ряда. А раз так, значит медиана - один из элементов нашего множества. Достаточно будет отсортировать наше множество и взять соответствующий элемент.\n",
    "\n",
    "Важно отметить, что будут элементы, которые не будут больше или меньше всех элементов нашего множества - они будут где-то между. В этот момент мы просто включим этот элемент в наше множество. Однако так как память у нас ограничена, то мы должны выкинуть какой-то элемент из множества, чтобы расход памяти не увеличивался. Легко заметить, что мы можем избавиться от минимального или максимального элемента нашего множества - если выкидываем максимум, то просто говорим, что на 1 увеличилось число элементов, больших чем наше (симметрично с минимумом). \n",
    "\n",
    "Осталось решить что выкинуть - минимум или максимум. Так как в конце мы бы хотели, чтобы больших и меньших элементов было примерно поровну, то тогда будем выкидывать элемент в соответствии с этим желанием - если меньших меньше, то выкидываем минимум, если больших - максимум.\n",
    "\n",
    "Более формальное описание алгоритма смотри здесь - https://www.cs.dartmouth.edu/~ac/Teach/data-streams-lecnotes.pdf\n",
    "\n",
    "Ниже - упрощенный схематичный пример работы алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Munro-Paterson](https://github.com/ADKosm/lsml-seminars-2020-public/blob/master/img/munro-paterson.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting median-stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile median-stream.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "from itertools import chain\n",
    "\n",
    "MEMORY = 5000\n",
    "A = {}\n",
    "\n",
    "def add(element, number):\n",
    "    A[element] = A.get(element, 0) + number\n",
    "    if A[element] == 0:\n",
    "        A.pop(element)\n",
    "\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    col = int(sys.argv[1])\n",
    "else:\n",
    "    col = 6\n",
    "    \n",
    "stream = map(lambda x: int(x[col]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "for _ in range(MEMORY):\n",
    "    add(next(stream), 1)\n",
    "\n",
    "A_min = min(A)\n",
    "A_max = max(A)\n",
    "\n",
    "larger = 0\n",
    "less = 0\n",
    "N = len(A)\n",
    "\n",
    "for element in stream:\n",
    "    N += 1\n",
    "    if element > A_max:\n",
    "        larger += 1\n",
    "    elif element < A_min:\n",
    "        less += 1\n",
    "    else:\n",
    "        if less < larger:\n",
    "            add(A_min, -1)\n",
    "            add(element, +1)\n",
    "            A_min = min(A)\n",
    "            less += 1\n",
    "        else:\n",
    "            add(A_max, -1)\n",
    "            add(element, +1)\n",
    "            A_max = max(A)\n",
    "            larger += 1\n",
    "\n",
    "if less < N / 2 and larger < N / 2:\n",
    "    median_index = N // 2 - less\n",
    "#     A = sorted(list(A))\n",
    "    A = list(chain.from_iterable([[value] * count for value, count in A.items()]))\n",
    "    result = A[median_index]\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"FAIL\")\n",
    "    print(N, less, larger, A_min, A_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:06<00:00, 650100.48it/s]\n",
      "FAIL\n",
      "4377630 3051379 1326220 3 3\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 median-stream.py 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:07<00:00, 610886.28it/s]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | shuf | tqdm --total 4382599 | python3 median-stream.py 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:07<00:00, 587079.58it/s]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | shuf | tqdm --total 4382599 | python3 median-stream.py 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:06<00:00, 675565.22it/s]\n",
      "FAIL\n",
      "4377748 1560787 2816812 64 64\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 median-stream.py 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:06<00:00, 637935.19it/s]\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | shuf | tqdm --total 4382599 | python3 median-stream.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что алгоритм работает только на потоках, которые хорошо перемешаны. Если же нам с данными не повезет, то все пропало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли находить медиану всегда с ограничением по памяти? Кажется, что эта задача близка к невыполнимой. Есть продвинутые алгоримты, которые всегда выдают ответ, однако их результат - лишь статистическая оценка, а не точный ответ. \n",
    "\n",
    "Например алгоритм **P-Square**. Он позволяет оценивать любые квантили на потоке. Подробнее можно почитать про него в оригинальной статье - https://www.cse.wustl.edu/~jain/papers/ftp/psqr.pdf . Сейчас останавливаться на нем мы не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скетчи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принадлежность множеству"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо подсчета каких-то статистик, часто возникает задача построить структуру данных, которая бы смогла отвечать нам на какие-то запросы после обработки потока.\n",
    "\n",
    "Например - **присутствовал ли такой элемент в потоке.**\n",
    "\n",
    "Если бы у нас было O(N) памяти, то тогда эту задачу можно было бы решить честно. Однако нам такой расклад не подходит, поэтому будем строить алгоритм, который отвечает на вопрос правильно с некоторой информацией, но при этом используя гораздо меньше памяти.\n",
    "\n",
    "Здесь нам могут помочь хеш-функции. Самая простая идея, которая нам может прийти в голову - запоминать не сами увиденные значения, а их хеши.\n",
    "\n",
    "План действий следующий: заведем массив размера T - здесь будем отмечать элементы, которые мы видели.\n",
    "Будем хешировать все элементы, которые есть в потоке, в отрезок [0, T] (T выбирается исходя из размера доступной памяти) и отмечать соответствующий элемент в нашем массиве как увиденный. После того, как мы обработаем таким образом весь массив мы можем обабатывать входящие запросы.\n",
    "\n",
    "Для входящего запроса посчитаем хеш элемента, который нас спросили и проверим, есть ли он у нас в массиве.\n",
    "Если в массиве указано, что такой хеш мы не видели, значит и сам элемент мы точно не видели - ответ нет.\n",
    "Если же указано, что видели - тогда возможно, что такой элемент присутствовал в потоке, а может и нет. Такое может произойти, когда хеш другого элемента из потока совпал с хешом элемента, про который спросили. В данной ситуации мы отвечаем, что видели, однако нужно держать в голове, что этот ответ может быть неверным с некоторой вероятностью.\n",
    "\n",
    "Для того, чтобы уменьшить вероятность ошибки, мы можешь применить следующий трюк - возьмем сразу P различных случайных хеш-функций. Будем отмечать в нашем массиве сразу все значения хешей, как увиденные.\n",
    "\n",
    "При ответе также возьмем все P хешей от элемента. Если хотя бы один из значений хеша отсутствует в нашем массиве, то значит такой элемент мы не видели.\n",
    "Если же все хеши присутствуют в массиве, значит или мы видели этот элемент, или нам очень не повезло и у нас случилось сразу P коллизий (что менее вероятно, чем в первом случае в одной хеш функцией).\n",
    "\n",
    "Структура данных, которую мы только что описали, называется **Bloom Filter**.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - https://shodhganga.inflibnet.ac.in/bitstream/10603/11703/9/09_chapter%204.pdf\n",
    "В статье также можно найти псевдокод работы алгоритма.\n",
    "\n",
    "Ниже - схема принципа работы струткуры.\n",
    "\n",
    "![Bloom-filter](https://github.com/ADKosm/lsml-seminars-2020-public/blob/master/img/bloom-filter.png?raw=true)\n",
    "\n",
    "<sub><sup>Картинка взята из https://en.wikipedia.org/wiki/Bloom_filter</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может возникнуть справедливый вопрос - где взять много случайных хеш-функций. Самый простой вариант - взять в качестве хеш-функции парамеризуемую функцию и случайно выставлять параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_hash(size):\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "    \n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "    \n",
    "    return _hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = generate_hash(20)\n",
    "h2 = generate_hash(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(h1(10))\n",
    "print(h2(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(h1(10))\n",
    "print(h1(21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[+1 доп. балл]**\n",
    "* Реализовать структуру bloom filter . Интерфейс алгоритма следующий - на stdin подается поток из датасета. Из потока необходимо выцепить колонку `REPORT_ID` (индекс 8) (для нее считаем bloom-filter). Путь до файла с запросами будет передан через аргументы командной строки. Необходимо обработать поток и после дать ответы на запросы из файла. Шаблон алгоритма прилагается ниже - необходимо дореализовать его (шаблон можно изменять как угодно - он приведен только для примера)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:21.845349Z",
     "start_time": "2020-02-19T19:35:21.724412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bloom-filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-filter.py\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "MEMORY = 5000\n",
    "\n",
    "\n",
    "def generate_hash(size):\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "\n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "\n",
    "    return _hash\n",
    "\n",
    "\n",
    "def main(query_file_path):\n",
    "    stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "    for element in stream:\n",
    "        pass # DO IT\n",
    "\n",
    "    with open(query_file_path, \"r\") as f:\n",
    "        for query in map(int, f):\n",
    "            print(\"NO\") # DO NOT FORGET TO ALSO UPDATE THIS\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('queries', metavar='query', type=str, nargs='?',\n",
    "                        help='path to queries file')\n",
    "    args = parser.parse_args()\n",
    "    main(args.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bloom-filter-query.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-filter-query.txt\n",
    "158324\n",
    "203546\n",
    "158776\n",
    "23\n",
    "894\n",
    "180926\n",
    "182984\n",
    "81511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:05<00:00, 840149.68it/s]\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 bloom-filter.py bloom-filter-query.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет частоты "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададимся немного более сложным вопросом - **сколько раз заданный элемент встречался в потоке.**\n",
    "\n",
    "Как и с просто проверкой наличия элемента в потоке - эту задачу не получится решать честно в заданных условиях. Ответ опять будет примерный.\n",
    "\n",
    "Попробуем продолжить идею использования хешей для решения этой задачи. Опять возьмем массив размера T в который будем записывать, сколько раз мы увидели тот или иной хеш. Для каждого нового элемента считаем хеш и увеличиваем соответствующий счетчик в массиве.\n",
    "\n",
    "Когда нам придет запрос - посчитаем хеш и посмотрим в массив. Число будет скорее всего больше, чем правильный ответ, так как из-за коллизий в соответствующую ячейку добавились результаты от элементов, которые имеют одинаковый хеш.\n",
    "\n",
    "Для того, чтобы уменьшить масштаб трагедии из-за этих коллизий опять воспользуемся приемом с несколькими хеш-функциями. Возьмем теперь сразу несколько массивов и для каждого массива возьмем свою случайную хеш-функцию.\n",
    "Для каждого массива будем проделывать такие же операции.\n",
    "\n",
    "Теперь когда к нам придет запрос - посчитаем всех хеши от элемента и посмотрим во все соответствующие ячейки в массивах. Все эти значения очевидно не меньше чем правильный ответ, а значит минимум из этих чисел - наиболее точная оценка того, сколько на самом деле раз мы видели этот элемент в потоке. Его и дадим в качестве ответа.\n",
    "\n",
    "Та конструкция, которую мы только что построили, называется **Count Min Sketch**.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - http://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture5.pdf.\n",
    "\n",
    "Ниже - схема принципа работы структуры.\n",
    "\n",
    "![count-min-sketch](https://raw.githubusercontent.com/ADKosm/lsml-seminars-2020-public/master/img/count-min-sketch.png)\n",
    "\n",
    "<sub><sup>Картика взята из https://github.com/gopalkrushnapattanaik/SystemDesign/wiki/Count-Min-Sketch</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[+1 доп. балл]**\n",
    "* Реализовать структуру count min sketch. Интерфейс такой же как и у bloom filter. Посчитать нужно все также поверх колонки `REPORT_ID`.  Путь до файла с запросами будет передан через аргументы командной строки. Шаблон прикладывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:32.237040Z",
     "start_time": "2020-02-19T19:35:32.119815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing count-min-sketch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile count-min-sketch.py\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "TABLE_SIZE = 5000\n",
    "HASHES_COUNT = 10\n",
    "\n",
    "\n",
    "def generate_hash(size):\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "\n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "\n",
    "    return _hash\n",
    "\n",
    "\n",
    "def main(query_file_path):\n",
    "    stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "    for element in stream:\n",
    "        pass # DO IT\n",
    "\n",
    "    with open(query_file_path, \"r\") as f:\n",
    "        for query in map(int, f):\n",
    "            print(0) # DO NOT FORGET TO ALSO UPDATE THIS\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('queries', metavar='query', type=str, nargs='?',\n",
    "                        help='path to queries file')\n",
    "    args = parser.parse_args()\n",
    "    main(args.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing count-min-sketch-query.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile count-min-sketch-query.txt\n",
    "158324\n",
    "203546\n",
    "158776\n",
    "23\n",
    "894\n",
    "180926\n",
    "182984\n",
    "81511\n",
    "187774\n",
    "201855\n",
    "190100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:05<00:00, 766573.11it/s]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 count-min-sketch.py count-min-sketch-query.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество уникальных элементов в потоке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из важный задач является подсчет **количества уникальных элементов в потоке.** В этот раз так просто составить табличку и просто хешировать в нее не получится. \n",
    "\n",
    "Самыми продвинутыми алгоритмами в этом классе считаются LogLog алгоритмы (LogLog, HyperLogLog, HLL++ и другие). Про них есть много литературы (например http://algo.inria.fr/flajolet/Publications/FlMa85.pdf ), но они используют зубодробительный тервер и разбирать их здесь мы не будем. Но важно знать, что такие алгоритмы есть и понимать, какую задачу они решают. \n",
    "\n",
    "На семинаре предлагается рассмотреть более простой, но все еще работающий способ решения задачи.\n",
    "\n",
    "Давайте посмотрим на двоичную запись хеша от элемента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1101001011011\n",
      "0b1100001111100\n",
      "0b1001111011001\n",
      "0b110001110010\n",
      "0b1001000111\n",
      "0b1110001101000\n",
      "0b110010110101\n",
      "0b10000101001110\n",
      "0b110000010011\n",
      "0b1101100100100\n"
     ]
    }
   ],
   "source": [
    "hash_function = generate_hash(10**4)\n",
    "\n",
    "for number in range(10):\n",
    "    print(bin(hash_function(number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на количество нулей в конце двоичной записи.\n",
    "Если хеш случайный, то вероятность того, что на конце будет 1 = 1/2. \n",
    "Вероятность того, что на конце будет 10 = 1/4. 100 - 1/8 и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1101001011011 0\n",
      "0b1100001111100 2\n",
      "0b1001111011001 0\n",
      "0b110001110010 1\n",
      "0b1001000111 0\n",
      "0b1110001101000 3\n",
      "0b110010110101 0\n",
      "0b10000101001110 1\n",
      "0b110000010011 0\n",
      "0b1101100100100 2\n"
     ]
    }
   ],
   "source": [
    "def zeros(number):\n",
    "    result = 0\n",
    "    while number and number & 1 == 0:\n",
    "        result += 1\n",
    "        number = number >> 1\n",
    "    return result\n",
    "\n",
    "for number in range(10):\n",
    "    print(bin(hash_function(number)), zeros(hash_function(number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это так же означает, что в множестве элементов примерно половина будет с нулем 0 на конце их хеш-функции, примерно четверть с одним 0 на конце, восьмая часть с двумя 0 на конце и тд. Этот факт нас очень скоро потребуется.\n",
    "\n",
    "Итак, если бы у нас было неограниченно памяти, мы бы в таком случае просто складывали все элементы из потока в множество и в конце просто бы посмотрели на размер этого множества - это был бы честный ответ в данной задаче.\n",
    "\n",
    "Учитывая, что память у нас ограничена, нам придется каким-то образом ужимать это множество. План следующий:\n",
    "\n",
    "* Возмем пустое множество B и отдельный счетчик z = 0\n",
    "* Для входящих элементов из потока будет добавлять в множество этот элемент.\n",
    "* Как только мы увидим, что размер множества превзошел определенный порог (лимит по памяти) производим следующую операцию\n",
    "  * Из множества удаляем все элементы у которых ровно z нулей на конце хеша (в первый раз будет 0 нулей, то есть те, у которых хеш оканчивается на 1)\n",
    "  * Увеличиваем z на 1\n",
    "* Далее все следующие элементы добавляем в множество, только если количество нулей на конце хеша не меньше z. \n",
    "* Как только в следующий раз у нас опять множество \"переполняется\", вновь повторяем процедуру с очисткой множества и увеличения z\n",
    "* В конце необходимо по нашему получившемуся множеству и z восстановить, сколько же различных элементов мы видели\n",
    "\n",
    "Каждая операция очистки множества удаляла из него примерно половину элементов (это мы увидели выше). Таким образом, исходное количество различных элементов в множестве будет равно примерно |B| * 2^z.\n",
    "\n",
    "Это и будет нашим ответом.\n",
    "\n",
    "Описанный алгоритм называется BJKST.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - http://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture5.pdf. В статье также можно найти псевдокод работы алгоритма.\n",
    "\n",
    "Ниже - схема принципа работы струткуры.\n",
    "\n",
    "![bjkst](https://github.com/ADKosm/lsml-seminars-2020-public/blob/master/img/bjkst.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[+1 доп. балл]**\n",
    "* Реализовать алгоритм BJKST. Из потока необходимо выцепить колонку `REPORT_ID` (для нее считаем количество уникальных элементов). Шаблон прикладывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:43.579368Z",
     "start_time": "2020-02-19T19:35:43.462118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bjkst.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bjkst.py\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "MEMORY = 5000\n",
    "\n",
    "def generate_hash(size):\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "\n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "\n",
    "    return _hash\n",
    "\n",
    "\n",
    "def zeros(number):\n",
    "    result = 0\n",
    "    while number and number & 1 == 0:\n",
    "        result += 1\n",
    "        number = number >> 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "    for element in stream:\n",
    "        pass # DO IT\n",
    "\n",
    "    result = 0\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:05<00:00, 811845.31it/s]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 bjkst.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что делать, если в потоке содержатся не числа, а строки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
